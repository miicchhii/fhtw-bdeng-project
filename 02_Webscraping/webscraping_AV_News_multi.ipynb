{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Webscraping of news articles",
   "id": "5afa294b65eb6cbd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load dependencies",
   "id": "302073137649506f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T18:11:44.626888Z",
     "start_time": "2025-06-24T18:11:44.347166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import glob\n",
    "import boto3\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import psutil"
   ],
   "id": "2c89e108c5c63b1a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load URLs from json files",
   "id": "a2339195f4261560"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Folder containing the files\n",
    "folder_path = \"alphavantage_news_json_selection\"\n",
    "\n",
    "# Get all files matching the pattern *_2024.json\n",
    "file_paths = glob.glob(os.path.join(folder_path, \"*_2024.json\"))\n",
    "\n",
    "all_relevance_scores = []\n",
    "score_url_pairs = []  # (score, url) tuples\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # Extract ticker from filename (e.g., AAPL from AAPL_2024.json)\n",
    "    ticker = os.path.basename(file_path).split(\"_\")[0]\n",
    "\n",
    "    # Load the data\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    # Collect relevance scores\n",
    "    ticker_relevance_scores = []\n",
    "\n",
    "    for article in articles:\n",
    "        article_url = article.get(\"url\")\n",
    "        for sentiment in article.get(\"ticker_sentiment\", []):\n",
    "            score_str = sentiment.get(\"relevance_score\")\n",
    "            if score_str is not None:\n",
    "                try:\n",
    "                    score = float(score_str)\n",
    "                    score_rounded = round(score, 2)\n",
    "                    ticker_relevance_scores.append(score_rounded)\n",
    "                    all_relevance_scores.append(score_rounded)\n",
    "                    score_url_pairs.append((score, article_url))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "    if not ticker_relevance_scores:\n",
    "        print(f\"No relevance scores found in {ticker}_2024.json\")\n",
    "        continue\n",
    "\n",
    "# Count frequencies\n",
    "total_counts = Counter(all_relevance_scores)\n",
    "\n",
    "# # Plot histogram\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.bar(total_counts.keys(), total_counts.values(), width=0.01)\n",
    "# plt.xlabel(\"Relevance Score\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.title(f\"Histogram of All Relevance Scores 2024\")\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "#\n",
    "# # Show or save the plot\n",
    "# plt.show()\n",
    "# # Or to save: plt.savefig(f\"{ticker}_2024_relevance_hist.png\")\n",
    "#\n",
    "# plt.close()\n",
    "\n",
    "print(f\"\\nüîó URLs: ({len(score_url_pairs)} found)\")"
   ],
   "id": "e36e63015953aebc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Webscraping using selenium",
   "id": "cc87187dbbd4b1aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "USER_AGENTS = [\n",
    "    # Chrome on Windows\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "\n",
    "    # Firefox on Windows\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0\",\n",
    "\n",
    "    # Edge on Windows\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0\",\n",
    "\n",
    "    # Chrome on macOS\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "\n",
    "    # Safari on macOS\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Safari/605.1.15\",\n",
    "\n",
    "    # Firefox on Linux\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:122.0) Gecko/20100101 Firefox/122.0\",\n",
    "]\n",
    "\n",
    "SCREEN_SIZES = [\n",
    "    \"1920,1080\",\n",
    "    \"1680,1050\",\n",
    "    \"1600,900\",\n",
    "    \"1440,900\",\n",
    "    \"1366,768\",\n",
    "    \"1280,1024\",\n",
    "    \"1024,768\"\n",
    "]\n",
    "\n",
    "OUTPUT_FOLDER = f\"{datetime.now().strftime('%Y%m%d')}_scraping_AV_texts_multi\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "filtered_urls = [url for score, url in score_url_pairs if score > 0.2]\n",
    "print(f\"\\nüîó URLs with relevance score > 0.2 ({len(filtered_urls)} found)\")\n",
    "\n",
    "def kill_orphan_chrome_processes():\n",
    "    for proc in psutil.process_iter(['name']):\n",
    "        if proc.info['name'] in ['chrome', 'chromedriver']:\n",
    "            try:\n",
    "                proc.kill()\n",
    "            except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "                pass\n",
    "\n",
    "def clean_html(raw_html):\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"iframe\", \"svg\", \"header\", \"footer\", \"nav\", \"aside\"]):\n",
    "        tag.decompose()\n",
    "    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "        comment.extract()\n",
    "    return str(soup)\n",
    "\n",
    "def get_driver():\n",
    "    user_agent = random.choice(USER_AGENTS)\n",
    "    screen_size = random.choice(SCREEN_SIZES)\n",
    "\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(f\"user-agent={user_agent}\")\n",
    "    options.add_argument(f\"window-size={screen_size}\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "\n",
    "def scrape_url(i, url):\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = get_driver()\n",
    "        domain = url.split(\"/\")[2]\n",
    "        print(f\"\\nüåê ({i}) Opening: {url}\")\n",
    "        driver.get(url)\n",
    "        wait = WebDriverWait(driver, 0.5)\n",
    "        time.sleep(2)\n",
    "\n",
    "        try:\n",
    "            cookie_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(., 'Accept')]\")))\n",
    "            cookie_button.click()\n",
    "            print(\"‚úÖ Cookie banner accepted\")\n",
    "            time.sleep(0.5)\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è No cookie banner appeared or could not be clicked\")\n",
    "\n",
    "        raw_html = driver.page_source\n",
    "        pure_html = clean_html(raw_html)\n",
    "\n",
    "        data = {\n",
    "            'url': url,\n",
    "            'html': pure_html,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        filename = f\"{OUTPUT_FOLDER}/{domain.replace('.', '_')}_{i}.json\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"üíæ Saved JSON: {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping {url}: {e}\")\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "        kill_orphan_chrome_processes()\n",
    "\n",
    "\n",
    "# --- Run Multithreaded Scraper ---\n",
    "MAX_WORKERS = 8  # Adjust depending on hardware resources\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    try:\n",
    "        last = i\n",
    "        next = i + 1\n",
    "    except NameError:\n",
    "        last = 0\n",
    "        next = last+1\n",
    "    futures = [executor.submit(scrape_url, i, url) for i, url in enumerate(filtered_urls[last:], start=next)]\n",
    "    for future in as_completed(futures):\n",
    "        pass  # Optionally handle result or exception here\n"
   ],
   "id": "2d3b6f946228d122"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Transfer raw scraping files to s3 server\n",
   "id": "44392bc4cd0bd53d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "s3_endpoint =\"http://172.29.16.105:9000\"\n",
    "s3_access_key = \"bdenggroup3\"\n",
    "s3_secret_key = \"bdenggroup3\"\n",
    "bucket_name= \"bdenggroup3\"\n",
    "\n",
    "s3 = boto3.client(\"s3\",\n",
    "                  endpoint_url=s3_endpoint,\n",
    "                  aws_access_key_id=s3_access_key,\n",
    "                  aws_secret_access_key=s3_secret_key,\n",
    "                  )\n",
    "\n",
    "# local_file_path = \"raw_sorted/www_benzinga_com_136.json\"\n",
    "# remote_file_path = f'{bucket_name}/www_benzinga_com_136.json'\n",
    "\n",
    "local_folder1 = \"raw_sorted\"\n",
    "local_folder2 = \"outstanding_scraping\"\n",
    "\n",
    "file_paths1 = glob.glob(os.path.join(local_folder1, \"*.json\"))\n",
    "file_paths2 = glob.glob(os.path.join(local_folder2, \"*.json\"))\n",
    "\n",
    "file_paths = file_paths1 + file_paths2\n",
    "\n",
    "i = 0\n",
    "\n",
    "for local_file_path in file_paths:\n",
    "    remote_file_path = f'raw/scrape_raw_{i}.json'\n",
    "    s3.upload_file(local_file_path, bucket_name, remote_file_path)\n",
    "    i+=1\n",
    "    if (i%100 ==0):\n",
    "        print (f'Checkpoint: {i} of {len(file_paths)} uploaded')\n",
    "\n"
   ],
   "id": "e4f8c37632ab81b3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
