{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "FOOL",
   "id": "d801f48e5dfce1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load your HTML string or file\n",
    "with open(\"fool.htm\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Find the main article container\n",
    "article_body = soup.find(\"div\", class_=\"article-body\")\n",
    "\n",
    "# Extract all text from <p> and <h2> tags inside it\n",
    "if article_body:\n",
    "    paragraphs = article_body.find_all(['p', 'h2'])\n",
    "    article_text = \"\\n\\n\".join(tag.get_text(strip=True) for tag in paragraphs)\n",
    "    print(article_text)\n",
    "else:\n",
    "    print(\"No article-body div found.\")\n"
   ],
   "id": "a1147743d0c7fe19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_article_text(html_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts readable article text from a Motley Fool-style HTML file.\n",
    "\n",
    "    Args:\n",
    "        html_path (str): Path to the HTML file.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned article text with paragraphs and section headings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(html_path, 'r', encoding='utf-8') as f:\n",
    "            html = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        article_body = soup.find(\"div\", class_=\"article-body\")\n",
    "\n",
    "        if article_body:\n",
    "            tags = article_body.find_all(['p', 'h2'])\n",
    "            text = \"\\n\\n\".join(tag.get_text(strip=True) for tag in tags)\n",
    "            return text\n",
    "        else:\n",
    "            return \"[!] No <div class='article-body'> found in the HTML.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"[!] Error reading or parsing file: {e}\"\n"
   ],
   "id": "71fc9e619b118a2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "text = extract_article_text(\"fool.htm\")\n",
    "print(text)\n"
   ],
   "id": "7dca2b8a5aa0398c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_article_text_from_json(json_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts article text from a JSON file that contains raw HTML under the 'html' key.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned article text or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        html = data.get('html')\n",
    "        if not html:\n",
    "            return \"[!] No 'html' key found in the JSON.\"\n",
    "\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        article_body = soup.find(\"div\", class_=\"article-body\")\n",
    "\n",
    "        if article_body:\n",
    "            tags = article_body.find_all(['p', 'h2'])\n",
    "            text = \"\\n\\n\".join(tag.get_text(strip=True) for tag in tags)\n",
    "            return text\n",
    "        else:\n",
    "            return \"[!] No <div class='article-body'> found in the HTML.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"[!] Error processing JSON file: {e}\"\n"
   ],
   "id": "843c1caca77b9585"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "article_text = extract_article_text_from_json(\"data/www_fool_com_54277.json\")\n",
    "print(article_text)\n"
   ],
   "id": "aa9f80af5735c49f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Fool Timestamp\n",
   "id": "e376b87cd2b48900"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T12:02:56.280555Z",
     "start_time": "2025-06-21T12:02:56.264412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime, timezone\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_fool_publish_date(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    meta_tag = soup.find(\"meta\", attrs={\"property\": \"article:published_time\"})\n",
    "    if meta_tag and meta_tag.get(\"content\"):\n",
    "        dt = datetime.fromisoformat(meta_tag[\"content\"])\n",
    "        dt_utc = dt.astimezone(timezone.utc)\n",
    "        return dt_utc.replace(tzinfo=None).isoformat(timespec=\"microseconds\")\n",
    "\n",
    "    return \"\"\n"
   ],
   "id": "5b481075e01d1b8f",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T12:06:26.956629Z",
     "start_time": "2025-06-21T12:06:26.731475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/scrape_raw_9999.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "html = data.get(\"html\", \"\")\n",
    "pubDate = extract_fool_publish_date(html)\n",
    "print(\"Publishing Date:\", pubDate)\n",
    "\n"
   ],
   "id": "d389b5689bcc4199",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publishing Date: 2024-03-14T13:33:13.000000\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BENZINGA",
   "id": "89f89dcc871dde13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_benzinga_article_text(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts main article text from Benzinga-style HTML, flattens for NLP.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Main content container for the article body\n",
    "    body_div = soup.find(\"div\", {\"id\": \"article-body\"})\n",
    "    if not body_div:\n",
    "        return \"\"\n",
    "\n",
    "    # Extract paragraphs and headings\n",
    "    tags = body_div.find_all([\"p\", \"h2\", \"li\"])\n",
    "    text_parts = [tag.get_text(strip=True) for tag in tags]\n",
    "\n",
    "    # Join and clean\n",
    "    raw_text = \" \".join(text_parts)\n",
    "    clean_text = re.sub(r'\\s+', ' ', raw_text).strip()\n",
    "\n",
    "    return clean_text\n"
   ],
   "id": "e293cc0ba3b9cdff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open(\"benzinga.htm\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "text = extract_benzinga_article_text(html_content)\n",
    "print(text[:5000])  # Preview first 500 chars\n"
   ],
   "id": "1c10af9ed02ad848"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T12:08:08.826952Z",
     "start_time": "2025-06-21T12:08:08.816409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_benzinga_publish_date(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the publishing date from Benzinga HTML assuming consistent format.\n",
    "    Example format: 'June 21, 2025 3:23 AM' â†’ ISO 8601.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    date_span = soup.find(\"span\", class_=\"article-date\")\n",
    "    if not date_span:\n",
    "        return \"\"\n",
    "\n",
    "    raw_date = date_span.get_text(strip=True)\n",
    "\n",
    "    # Parse with fixed format\n",
    "    dt = datetime.strptime(raw_date, \"%B %d, %Y %I:%M %p\")\n",
    "    return dt.isoformat()\n"
   ],
   "id": "37a56444ff72178d",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T12:10:18.698101Z",
     "start_time": "2025-06-21T12:10:18.457952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"benzinga.htm\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "text = extract_benzinga_publish_date(html_content)\n",
    "print(text)  # Prev"
   ],
   "id": "d11fe5ca0315814a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-01T12:00:00\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_zacks_article_text(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts main article text from Zacks-style HTML and removes promotional ad blocks.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    body_div = soup.find(\"div\", {\"id\": \"comtext\"})\n",
    "    if not body_div:\n",
    "        return \"\"\n",
    "\n",
    "    # Extract paragraph, heading, and list elements\n",
    "    tags = body_div.find_all([\"p\", \"h2\", \"li\"])\n",
    "    text_parts = [tag.get_text(strip=True) for tag in tags]\n",
    "\n",
    "    # Combine into a single string\n",
    "    raw_text = \" \".join(text_parts)\n",
    "    clean_text = re.sub(r'\\s+', ' ', raw_text).strip()\n",
    "\n",
    "    # Remove text between known ad break markers\n",
    "    ad_pattern = r'-{5,}.*?-{5,}'\n",
    "    clean_text = re.sub(ad_pattern, '', clean_text)\n",
    "\n",
    "    return clean_text.strip()\n",
    "\n",
    "\n"
   ],
   "id": "9a412c16fb0ec0bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test locally with saved file\n",
    "with open(\"zacks.htm\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "text = extract_zacks_article_text(html_content)\n",
    "print(text)  # Preview first 5000 chars"
   ],
   "id": "d7f19d471ca7598d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# FINAL\n",
   "id": "c4b899f8c032f276"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Imports",
   "id": "69acc049c3a02a3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n"
   ],
   "id": "4d6abaa84115d8e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## HTML Extraction Helper\n",
    "fool"
   ],
   "id": "a1f3d598dd18bce6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_fool_article_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    article_body = soup.find(\"div\", class_=\"article-body\")\n",
    "    if article_body:\n",
    "        tags = article_body.find_all(['p', 'h2'])\n",
    "        flat_text = ' '.join(tag.get_text(strip=True) for tag in tags)\n",
    "        return re.sub(r'\\s+', ' ', flat_text).strip()\n",
    "    return \"\"\n"
   ],
   "id": "960d1dfbddd44145"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "benzinga",
   "id": "c43d41f03be750bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def extract_benzinga_article_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    article_body = soup.find(\"div\", id=\"article-body\")\n",
    "    if article_body:\n",
    "        tags = article_body.find_all(['p', 'h2', 'li'])\n",
    "        flat_text = ' '.join(tag.get_text(strip=True) for tag in tags)\n",
    "        return re.sub(r'\\s+', ' ', flat_text).strip()\n",
    "    return \"\""
   ],
   "id": "50f7f1084d7f2096"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "dynamic dispatcher\n",
   "id": "b12c6f531acaa27b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_article_text_by_source(url: str, html: str) -> str:\n",
    "    if \"fool.com\" in url:\n",
    "        return extract_fool_article_text(html)\n",
    "    elif \"benzinga.com\" in url:\n",
    "        return extract_benzinga_article_text(html)\n",
    "    else:\n",
    "        return \"\"\n"
   ],
   "id": "a9cb2b4e95114961"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "full file processing",
   "id": "4fa01966299ed526"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def process_json_file(input_path: str) -> str:\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        html = data.get(\"html\", \"\")\n",
    "        url = data.get(\"url\", \"\")\n",
    "        scraping_timestamp = data.get(\"timestamp\", \"\")\n",
    "\n",
    "        article_text = extract_article_text_by_source(url, html)\n",
    "        parsed_data = {\n",
    "            \"url\": url,\n",
    "            \"scrapingTimestamp\": scraping_timestamp,\n",
    "            \"parsingTimestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"articleText\": article_text\n",
    "        }\n",
    "\n",
    "        # Create the new filename\n",
    "        dirname, filename = os.path.split(input_path)\n",
    "        cleaned_filename = filename.replace(\"scrape_raw_\", \"scrape_cleaned_\")\n",
    "        output_path = os.path.join(dirname, cleaned_filename)\n",
    "\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(parsed_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        return output_path\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error processing {input_path}: {e}\"\n",
    "\n"
   ],
   "id": "c73bfa0c5bbce548"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Batch Processing",
   "id": "299acd5a6042c453"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def process_all_json_files(folder_path: str):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".json\") and not filename.endswith(\"_parsed.json\"):\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            output = process_json_file(full_path)\n",
    "            print(f\"âœ… Processed: {filename} â†’ {output}\")\n"
   ],
   "id": "86748cc2225c8992"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run Function",
   "id": "f83603b6127373d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TODO: Adjust path\n",
    "process_all_json_files(\"./data\")\n"
   ],
   "id": "e81aa4d2b7dd3636"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
