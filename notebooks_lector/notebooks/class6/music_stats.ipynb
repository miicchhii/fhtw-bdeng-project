{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c8f425-aa7f-46cf-a65e-3e188538cd1f",
   "metadata": {},
   "source": [
    "# Kafka music stream analysis\n",
    "\n",
    "Let's first take a look at the data provided by Kafla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cacb7faf-20c0-4389-b4f0-4a0e7e6fbf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka.admin import AdminClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a6570c3-b946-47b7-bddd-fdb6227273fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {'bootstrap.servers': '172.29.16.101:9092'}\n",
    "\n",
    "kadmin = AdminClient(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f96e4f-51b9-4989-b0a4-aaf232d2ce73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'here-routes': TopicMetadata(here-routes, 1 partitions),\n",
       " 'personenkraftwagen_data': TopicMetadata(personenkraftwagen_data, 1 partitions),\n",
       " 'traffic_data_group7': TopicMetadata(traffic_data_group7, 1 partitions),\n",
       " 'weather_topic3': TopicMetadata(weather_topic3, 1 partitions),\n",
       " 'weather_topic2': TopicMetadata(weather_topic2, 1 partitions),\n",
       " 'stock-prices': TopicMetadata(stock-prices, 1 partitions),\n",
       " 'hello-world': TopicMetadata(hello-world, 1 partitions),\n",
       " 'personenkraftwagen': TopicMetadata(personenkraftwagen, 1 partitions),\n",
       " 'stocks': TopicMetadata(stocks, 1 partitions),\n",
       " 'emissions_topic': TopicMetadata(emissions_topic, 1 partitions),\n",
       " 'music': TopicMetadata(music, 1 partitions),\n",
       " 'delhi_routes': TopicMetadata(delhi_routes, 1 partitions),\n",
       " 'roulette': TopicMetadata(roulette, 1 partitions),\n",
       " 'traffic-data': TopicMetadata(traffic-data, 1 partitions),\n",
       " 'weather_topic4': TopicMetadata(weather_topic4, 1 partitions),\n",
       " 'nyt_article_publishes': TopicMetadata(nyt_article_publishes, 1 partitions),\n",
       " 'emissions': TopicMetadata(emissions, 1 partitions),\n",
       " 'here-api-routes': TopicMetadata(here-api-routes, 1 partitions),\n",
       " 'wiki-changes': TopicMetadata(wiki-changes, 1 partitions),\n",
       " 'btc_price': TopicMetadata(btc_price, 1 partitions),\n",
       " 'air_quality_data': TopicMetadata(air_quality_data, 1 partitions),\n",
       " 'game_scores': TopicMetadata(game_scores, 1 partitions),\n",
       " 'london_routes': TopicMetadata(london_routes, 1 partitions),\n",
       " 'yahoo-finance': TopicMetadata(yahoo-finance, 1 partitions),\n",
       " 'weather_topic': TopicMetadata(weather_topic, 1 partitions),\n",
       " 'crypto': TopicMetadata(crypto, 1 partitions),\n",
       " 'wikimedia-changes': TopicMetadata(wikimedia-changes, 1 partitions),\n",
       " 'bitcoin_price_data': TopicMetadata(bitcoin_price_data, 1 partitions),\n",
       " 'testTopic': TopicMetadata(testTopic, 1 partitions),\n",
       " 'berlin_routes': TopicMetadata(berlin_routes, 1 partitions),\n",
       " 'here_api_data': TopicMetadata(here_api_data, 1 partitions),\n",
       " 'hello': TopicMetadata(hello, 1 partitions),\n",
       " 'co2_emissions': TopicMetadata(co2_emissions, 1 partitions),\n",
       " '__consumer_offsets': TopicMetadata(__consumer_offsets, 50 partitions)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kadmin.list_topics().topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bbfff7-d936-421f-900c-d960f40f5ae2",
   "metadata": {},
   "source": [
    "Let's take a closer look at the ```music``` topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5877127-8a23-4c7e-bbcd-658c0c489aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98499883-5e71-47f6-b0e0-10ad4f4f7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {'bootstrap.servers': '172.29.16.101:9092',\n",
    "        'group.id': 'music-stats'}\n",
    "\n",
    "consumer = Consumer(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32b47ea5-5315-4272-b8e1-72cc782972e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"ts\": 1716379588091, \"auth\": \"Logged In\", \"page\": \"NextSong\", \"song\": \"Loving You Sunday Morning\", \"level\": \"paid\", \"artist\": \"Scorpions\", \"gender\": \"M\", \"method\": \"PUT\", \"status\": 200, \"userId\": \"7\", \"lastName\": \"Freeman\", \"location\": \"Bakersfield, CA\", \"track_id\": 3291, \"firstName\": \"Colin\", \"sessionId\": 784, \"userAgent\": \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0\", \"registration\": 1715218288033, \"itemInSession\": 25}'\n"
     ]
    }
   ],
   "source": [
    "consumer.subscribe(['music'])\n",
    "\n",
    "while True:\n",
    "    msg = consumer.poll(timeout=1.0)\n",
    "        \n",
    "    if msg is not None and msg.error():\n",
    "        if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "            # End of partition event\n",
    "            sys.stderr.write('%% %s [%d] reached end at offset %d\\n' %\n",
    "                             (msg.topic(), msg.partition(), msg.offset()))\n",
    "        elif msg.error():\n",
    "            raise KafkaException(msg.error())\n",
    "    elif msg is not None:\n",
    "        print(msg.value())\n",
    "        break\n",
    "    else:\n",
    "        print('No msg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447e6943-d291-45d9-9faf-7d8902605b02",
   "metadata": {},
   "source": [
    "## Spark Streaming and Kafka\n",
    "\n",
    "Now that we know the structure of the streamed data, we can analyze the stream with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e7e647b-b326-46f5-bb28-7d63ad401863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/28 10:18:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Kafka Music Stats\") \\\n",
    "    .master(\"spark://172.29.16.102:7077\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36d1e4d-51af-4331-aabf-bef7101f2a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"172.29.16.101:9092\") \\\n",
    "    .option(\"subscribe\", \"music\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e30d680-693a-4b69-aa33-471884673b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType() \\\n",
    "    .add(\"artist\", StringType()) \\\n",
    "    .add(\"status\", StringType()) \\\n",
    "    .add(\"level\", StringType()) \\\n",
    "    .add(\"registration\", TimestampType()) \\\n",
    "    .add(\"ts\", TimestampType())\n",
    "\n",
    "# Parse JSON messages and extract fields\n",
    "music_df = music_stream \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(\"value\", schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f94be04-593c-4b3f-9a47-6623bdba5323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- registration: timestamp (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "music_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcc9e3a-3672-4b8c-95e3-969d53c5310f",
   "metadata": {},
   "source": [
    "### Total Artist Streamed Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "860ba5ea-836a-4947-abcb-091a0e8bfeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_counts = music_df.groupBy(\"artist\").count().sort(desc('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e8ff5da-19bd-4303-85df-fcd97eb9c200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 09:37:25 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-86687fc5-72de-4cdf-a5e6-2cabbc76d149. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/01/28 09:37:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/01/28 09:37:25 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+-----+\n",
      "|artist|count|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------------+-----+\n",
      "|              artist|count|\n",
      "+--------------------+-----+\n",
      "|         Iron Maiden|  213|\n",
      "|                  U2|  135|\n",
      "|        Led Zeppelin|  114|\n",
      "|           Metallica|  112|\n",
      "|                Lost|   92|\n",
      "|         Deep Purple|   92|\n",
      "|           Pearl Jam|   67|\n",
      "|       Lenny Kravitz|   57|\n",
      "|    Various artistss|   56|\n",
      "|          The Office|   53|\n",
      "|       Faith No More|   52|\n",
      "|           Van Halen|   52|\n",
      "|Os Paralamas Do S...|   49|\n",
      "|        Eric Clapton|   48|\n",
      "|Red Hot Chili Pep...|   48|\n",
      "|               Queen|   45|\n",
      "|        Foo Fighters|   44|\n",
      "|      Guns N'' Roses|   42|\n",
      "|              R.E.M.|   41|\n",
      "|  The Rolling Stones|   41|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = artist_counts \\\n",
    "    .writeStream \\\n",
    "    .outputMode('complete') \\\n",
    "    .format('console') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f20a3280-1c61-479e-9f2a-13cffe46787c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 09:37:43 WARN TaskSetManager: Lost task 0.0 in stage 67.0 (TID 5712) (172.29.16.108 executor 1): TaskKilled (Stage cancelled: Job 30 cancelled part of cancelled job group 600017d8-90eb-4bf5-ac6a-cfab4a337171)\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2390d0-e309-4eca-b5d6-43d5038d4904",
   "metadata": {},
   "source": [
    "### Registation Level Count in 30 day windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "415066a5-563d-49a0-9f17-d6288e129b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_level_counts = music_df.groupBy(\n",
    "    window(col('registration'), '30 days'),\n",
    "    col('level')\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d77b4db7-46d6-471a-b9da-7d2e1eeae6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 10:26:04 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1f11d0b2-1d88-4e9f-b087-85c0cf385687. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/01/28 10:26:04 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/01/28 10:26:04 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = windowed_level_counts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"status_stats\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4617fbef-4440-4ef6-876b-5f305cd3fdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 236:===========> (174 + 4) / 200][Stage 237:>                (0 + 0) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+-----+-----+\n",
      "|window                                        |level|count|\n",
      "+----------------------------------------------+-----+-----+\n",
      "|{+56355-10-05 00:00:00, +56355-11-04 00:00:00}|free |201  |\n",
      "|{+56223-07-09 00:00:00, +56223-08-08 00:00:00}|paid |200  |\n",
      "|{+56328-02-29 00:00:00, +56328-03-30 00:00:00}|paid |200  |\n",
      "|{+56335-07-22 00:00:00, +56335-08-21 00:00:00}|paid |200  |\n",
      "|{+56242-12-26 00:00:00, +56243-01-25 00:00:00}|paid |161  |\n",
      "|{+56160-07-08 00:00:00, +56160-08-07 00:00:00}|paid |161  |\n",
      "|{+56151-03-28 00:00:00, +56151-04-27 00:00:00}|paid |160  |\n",
      "|{+56142-07-13 00:00:00, +56142-08-12 00:00:00}|free |160  |\n",
      "|{+56102-06-13 00:00:00, +56102-07-13 00:00:00}|paid |160  |\n",
      "|{+56347-11-16 00:00:00, +56347-12-16 00:00:00}|paid |160  |\n",
      "|{+56274-06-11 00:00:00, +56274-07-11 00:00:00}|paid |160  |\n",
      "|{+56190-12-28 00:00:00, +56191-01-27 00:00:00}|free |160  |\n",
      "|{+56216-10-13 00:00:00, +56216-11-12 00:00:00}|free |160  |\n",
      "|{+56297-02-10 00:00:00, +56297-03-12 00:00:00}|paid |122  |\n",
      "|{+56163-07-23 00:00:00, +56163-08-22 00:00:00}|free |122  |\n",
      "|{+56093-07-29 00:00:00, +56093-08-28 00:00:00}|paid |121  |\n",
      "|{+56210-07-17 00:00:00, +56210-08-16 00:00:00}|free |121  |\n",
      "|{+56243-03-26 00:00:00, +56243-04-25 00:00:00}|paid |121  |\n",
      "|{+56154-03-12 00:00:00, +56154-04-11 00:00:00}|free |121  |\n",
      "|{+56176-05-15 00:00:00, +56176-06-14 00:00:00}|free |121  |\n",
      "+----------------------------------------------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM status_stats ORDER BY count DESC\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10276ef4-a9db-40bd-843f-d5859c007dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 10:29:42 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 46, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@553536ee] is aborting.\n",
      "25/01/28 10:29:42 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 46, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@553536ee] aborted.\n",
      "25/01/28 10:29:43 WARN TaskSetManager: Lost task 0.0 in stage 240.0 (TID 22436) (172.29.16.108 executor 1): TaskKilled (Stage cancelled: Job 127 cancelled part of cancelled job group 8cafec77-8d91-42ff-b7e7-5d966fbf7bae)\n"
     ]
    }
   ],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d52050a-96da-4800-92de-6e7d5c36f7d4",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09a81171-4f71-45e6-877a-34e41fd2c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac49a0f-5352-42c9-95ae-054edf41ec3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
