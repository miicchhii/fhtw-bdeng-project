{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sentiment Analyse",
   "id": "df81f3653d751a74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Kafka Producer",
   "id": "7ab8cf48294b9d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import boto3\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka import KafkaProducer\n",
    "from kafka.errors import TopicAlreadyExistsError\n",
    "\n",
    "# MinIO-Verbindung\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://172.29.16.105:9000',\n",
    "    aws_access_key_id='bdenggroup3',\n",
    "    aws_secret_access_key='bdenggroup3'\n",
    ")\n",
    "\n",
    "bucket_name = 'bdenggroup3' # S3 bucket\n",
    "prefix = 'parsed/'          # S3 folder in bucket\n",
    "\n",
    "# Kafka-Einstellungen\n",
    "kafka_broker = 'localhost:9092'\n",
    "topic_name = 'artikel-sentiment'\n",
    "\n",
    "# Kafka-Topic erstellen, falls es nicht existiert\n",
    "admin_client = KafkaAdminClient(bootstrap_servers=kafka_broker)\n",
    "try:\n",
    "    admin_client.create_topics([NewTopic(name=topic_name, num_partitions=1, replication_factor=1)])\n",
    "    print(f\"‚úÖ Kafka-Topic '{topic_name}' wurde erstellt.\")\n",
    "except TopicAlreadyExistsError:\n",
    "    print(f\"‚ÑπÔ∏è Kafka-Topic '{topic_name}' existiert bereits.\")\n",
    "\n",
    "# Kafka-Producer initialisieren\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=kafka_broker,\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Alle Dateien im Ordner 'parsed/' aus MinIO lesen und senden\n",
    "paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "for page in page_iterator:\n",
    "    for obj in page.get(\"Contents\", []):\n",
    "        key = obj[\"Key\"]\n",
    "        if not key.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        content = s3.get_object(Bucket=bucket_name, Key=key)[\"Body\"].read().decode(\"utf-8\")\n",
    "        parsed_article = json.loads(content)\n",
    "        # Extrahiere Dateinamen, z.B. parsed_255.json\n",
    "        filename = key.split(\"/\")[-1]\n",
    "\n",
    "        # F√ºge `source`-Feld hinzu\n",
    "        message = {\n",
    "            \"source\": filename,\n",
    "            \"url\": parsed_article.get(\"url\"),\n",
    "            \"articleText\": parsed_article.get(\"articleText\"),\n",
    "            \"articleTimestamp\": parsed_article.get(\"articleTimestamp\"),\n",
    "            \"scrapingTimestamp\": parsed_article.get(\"scrapingTimestamp\"),\n",
    "            \"parsingTimestamp\": parsed_article.get(\"parsingTimestamp\")\n",
    "        }\n",
    "\n",
    "        producer.send(topic_name, message)\n",
    "        print(f\"üì§ Gesendet: {key}\")\n",
    "\n",
    "producer.flush()\n",
    "print(\"‚úÖ Alle Artikel wurden an Kafka gesendet.\")"
   ],
   "id": "f67ad5a87ebdc9dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Consumer mit Multithreading",
   "id": "47fd214370996aa6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "processed = []",
   "id": "2b83017ebdc82370",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import boto3\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from kafka import KafkaConsumer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup\n",
    "nltk.download(\"punkt\")\n",
    "sentence_tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "# Ticker + Synonyme laden\n",
    "ticker_df = pd.read_csv(\"ticker_synonyme.csv\")\n",
    "ticker_map = defaultdict(set)\n",
    "for _, row in ticker_df.iterrows():\n",
    "    ticker_map[row[\"ticker\"]].add(row[\"synonym\"].lower())\n",
    "\n",
    "# FinBERT\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# ü™£ MinIO\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://172.29.16.105:9000',\n",
    "    aws_access_key_id='bdenggroup3',\n",
    "    aws_secret_access_key='bdenggroup3'\n",
    ")\n",
    "\n",
    "bucket_name = 'bdenggroup3'\n",
    "output_prefix = 'sentiment/'\n",
    "\n",
    "# üîß Konfiguration\n",
    "block_size = 3\n",
    "min_blocks_per_ticker = 2\n",
    "min_relevance_score = 0.1\n",
    "max_workers = 4\n",
    "\n",
    "# Kafka\n",
    "consumer = KafkaConsumer(\n",
    "    \"artikel-sentiment\",\n",
    "    bootstrap_servers=\"localhost:9092\",\n",
    "    auto_offset_reset=\"earliest\",\n",
    "    enable_auto_commit=False,\n",
    "    group_id=\"finbert-consumer\",\n",
    "    value_deserializer=lambda m: json.loads(m.decode(\"utf-8\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Verarbeitung eines Artikels\n",
    "def process_article(article):\n",
    "    try:\n",
    "        source_filename = article.get(\"source\", \"parsed_unknown.json\")\n",
    "        sentiment_filename = source_filename.replace(\"parsed_\", \"sentiment_\")\n",
    "        key = f\"{output_prefix}{sentiment_filename}\"\n",
    "\n",
    "        # üö´ Duplikate √ºberspringen\n",
    "        try:\n",
    "            s3.head_object(Bucket=bucket_name, Key=key)\n",
    "            print(f\"‚è≠Ô∏è Bereits verarbeitet, wird √ºbersprungen: {key}\")\n",
    "            return\n",
    "        except s3.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] != '404':\n",
    "                raise\n",
    "\n",
    "        text = article.get(\"articleText\")\n",
    "        if not text:\n",
    "            print(f\"‚ö†Ô∏è Kein Text vorhanden f√ºr: {article.get('source')}\")\n",
    "            return\n",
    "\n",
    "        s√§tze = sentence_tokenizer.tokenize(text)\n",
    "        if len(s√§tze) == 0:\n",
    "            return\n",
    "\n",
    "        bl√∂cke = [s√§tze[i:i+block_size] for i in range(0, len(s√§tze), block_size)]\n",
    "        block_sentences = [\" \".join(block) for block in bl√∂cke]\n",
    "        total_blocks = len(bl√∂cke)\n",
    "\n",
    "        block_sentiments = classifier(block_sentences, truncation=True)\n",
    "\n",
    "        ticker_to_blocks = defaultdict(list)\n",
    "        for idx, block in enumerate(bl√∂cke):\n",
    "            block_text = \" \".join(block).lower()\n",
    "            for ticker, syns in ticker_map.items():\n",
    "                if any(syn in block_text for syn in syns):\n",
    "                    ticker_to_blocks[ticker].append(idx)\n",
    "\n",
    "        ticker_to_blocks = {\n",
    "            ticker: idxs for ticker, idxs in ticker_to_blocks.items()\n",
    "            if len(idxs) >= min_blocks_per_ticker\n",
    "        }\n",
    "\n",
    "        if not ticker_to_blocks:\n",
    "            print(f\"‚ÑπÔ∏è Keine ausreichend relevanten Ticker f√ºr: {article.get('source')}\")\n",
    "            return\n",
    "\n",
    "        tickers_output = []\n",
    "        for ticker, idx_list in ticker_to_blocks.items():\n",
    "            sentiment_sum = 0\n",
    "            score_sum = 0\n",
    "            for idx in idx_list:\n",
    "                result = block_sentiments[idx]\n",
    "                weight = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}[result[\"label\"]]\n",
    "                sentiment_sum += weight * result[\"score\"]\n",
    "                score_sum += result[\"score\"]\n",
    "\n",
    "            weighted_sentiment = sentiment_sum / score_sum if score_sum else 0\n",
    "            relevance_score = len(idx_list) / total_blocks if total_blocks else 0\n",
    "\n",
    "            if relevance_score >= min_relevance_score:\n",
    "                tickers_output.append({\n",
    "                    \"ticker\": ticker,\n",
    "                    \"sentiment_score\": round(weighted_sentiment, 3),\n",
    "                    \"relevance_score\": round(relevance_score, 3)\n",
    "                })\n",
    "\n",
    "        if not tickers_output:\n",
    "            print(f\"‚ÑπÔ∏è Keine Ticker mit ausreichender Relevanz f√ºr: {article.get('source')}\")\n",
    "            return\n",
    "\n",
    "        ergebnis = {\n",
    "            \"url\": article.get(\"url\"),\n",
    "            \"articleTimestamp\": article.get(\"articleTimestamp\"),\n",
    "            \"scrapingTimestamp\": article.get(\"scrapingTimestamp\"),\n",
    "            \"parsingTimestamp\": article.get(\"parsingTimestamp\"),\n",
    "            \"sentimentTimestamp\": datetime.utcnow().isoformat(),\n",
    "            \"tickers\": tickers_output\n",
    "        }\n",
    "\n",
    "        s3.put_object(Bucket=bucket_name, Key=key, Body=json.dumps(ergebnis).encode(\"utf-8\"))\n",
    "        print(f\"‚úÖ Sentiment f√ºr {len(tickers_output)} Ticker gespeichert: {key}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler bei Artikelverarbeitung: {e}\")\n",
    "\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# Haupt-Loop mit Batching & Parallelit√§t\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    while True:\n",
    "        batch = consumer.poll(timeout_ms=1000, max_records=10)\n",
    "\n",
    "        futures = []\n",
    "        for _, messages in batch.items():\n",
    "            for msg in messages:\n",
    "                counter += 1\n",
    "                if msg not in processed:\n",
    "                    processed.append(msg)\n",
    "                    print(f'Processing message Nr. {counter}')\n",
    "                    futures.append(executor.submit(process_article, msg.value))\n",
    "                else:\n",
    "                    print(f'Message Nr. {counter} already processed, skipping')\n",
    "\n",
    "        # Warten, bis alle fertig\n",
    "        for f in futures:\n",
    "            f.result()\n",
    "\n",
    "        # Nach Erfolg Kafka-Offsets committen\n",
    "        consumer.commit()"
   ],
   "id": "42185cff780ecb7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Multithreaded on GPU",
   "id": "dbe651ec8ca39c9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.get_device_name(0))\n"
   ],
   "id": "cdb8feae23f046c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import boto3\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from kafka import KafkaConsumer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "\n",
    "# Setup\n",
    "nltk.download(\"punkt\")\n",
    "sentence_tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "# Ticker + Synonyme laden\n",
    "ticker_df = pd.read_csv(\"ticker_synonyme.csv\")\n",
    "ticker_map = defaultdict(set)\n",
    "for _, row in ticker_df.iterrows():\n",
    "    ticker_map[row[\"ticker\"]].add(row[\"synonym\"].lower())\n",
    "\n",
    "# FinBERT\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# ü™£ MinIO\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://172.29.16.105:9000',\n",
    "    aws_access_key_id='bdenggroup3',\n",
    "    aws_secret_access_key='bdenggroup3'\n",
    ")\n",
    "\n",
    "bucket_name = 'bdenggroup3'\n",
    "output_prefix = 'sentiment/'\n",
    "\n",
    "# üîß Konfiguration\n",
    "block_size = 3\n",
    "min_blocks_per_ticker = 2\n",
    "min_relevance_score = 0.1\n",
    "max_workers = 4\n",
    "\n",
    "# Kafka\n",
    "consumer = KafkaConsumer(\n",
    "    \"artikel-sentiment\",\n",
    "    bootstrap_servers=\"localhost:9092\",\n",
    "    auto_offset_reset=\"earliest\",\n",
    "    enable_auto_commit=False,\n",
    "    group_id=\"finbert-consumer\",\n",
    "    value_deserializer=lambda m: json.loads(m.decode(\"utf-8\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Verarbeitung eines Artikels\n",
    "def process_article(article):\n",
    "    try:\n",
    "        source_filename = article.get(\"source\", \"parsed_unknown.json\")\n",
    "        sentiment_filename = source_filename.replace(\"parsed_\", \"sentiment_\")\n",
    "        key = f\"{output_prefix}{sentiment_filename}\"\n",
    "\n",
    "        # üö´ Duplikate √ºberspringen\n",
    "        try:\n",
    "            s3.head_object(Bucket=bucket_name, Key=key)\n",
    "            print(f\"‚è≠Ô∏è Bereits verarbeitet, wird √ºbersprungen: {key}\")\n",
    "            return\n",
    "        except s3.exceptions.ClientError as e:\n",
    "            if e.response['Error']['Code'] != '404':\n",
    "                raise\n",
    "\n",
    "        text = article.get(\"articleText\")\n",
    "        if not text:\n",
    "            print(f\"‚ö†Ô∏è Kein Text vorhanden f√ºr: {article.get('source')}\")\n",
    "            return\n",
    "\n",
    "        s√§tze = sentence_tokenizer.tokenize(text)\n",
    "        if len(s√§tze) == 0:\n",
    "            return\n",
    "\n",
    "        bl√∂cke = [s√§tze[i:i+block_size] for i in range(0, len(s√§tze), block_size)]\n",
    "        block_sentences = [\" \".join(block) for block in bl√∂cke]\n",
    "        total_blocks = len(bl√∂cke)\n",
    "\n",
    "        block_sentiments = classifier(block_sentences, truncation=True)\n",
    "\n",
    "        ticker_to_blocks = defaultdict(list)\n",
    "        for idx, block in enumerate(bl√∂cke):\n",
    "            block_text = \" \".join(block).lower()\n",
    "            for ticker, syns in ticker_map.items():\n",
    "                if any(syn in block_text for syn in syns):\n",
    "                    ticker_to_blocks[ticker].append(idx)\n",
    "\n",
    "        ticker_to_blocks = {\n",
    "            ticker: idxs for ticker, idxs in ticker_to_blocks.items()\n",
    "            if len(idxs) >= min_blocks_per_ticker\n",
    "        }\n",
    "\n",
    "        if not ticker_to_blocks:\n",
    "            print(f\"‚ÑπÔ∏è Keine ausreichend relevanten Ticker f√ºr: {article.get('source')}\")\n",
    "            return\n",
    "\n",
    "        tickers_output = []\n",
    "        for ticker, idx_list in ticker_to_blocks.items():\n",
    "            sentiment_sum = 0\n",
    "            score_sum = 0\n",
    "            for idx in idx_list:\n",
    "                result = block_sentiments[idx]\n",
    "                weight = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}[result[\"label\"]]\n",
    "                sentiment_sum += weight * result[\"score\"]\n",
    "                score_sum += result[\"score\"]\n",
    "\n",
    "            weighted_sentiment = sentiment_sum / score_sum if score_sum else 0\n",
    "            relevance_score = len(idx_list) / total_blocks if total_blocks else 0\n",
    "\n",
    "            if relevance_score >= min_relevance_score:\n",
    "                tickers_output.append({\n",
    "                    \"ticker\": ticker,\n",
    "                    \"sentiment_score\": round(weighted_sentiment, 3),\n",
    "                    \"relevance_score\": round(relevance_score, 3)\n",
    "                })\n",
    "\n",
    "        if not tickers_output:\n",
    "            print(f\"‚ÑπÔ∏è Keine Ticker mit ausreichender Relevanz f√ºr: {article.get('source')}\")\n",
    "            return\n",
    "\n",
    "        ergebnis = {\n",
    "            \"url\": article.get(\"url\"),\n",
    "            \"articleTimestamp\": article.get(\"articleTimestamp\"),\n",
    "            \"scrapingTimestamp\": article.get(\"scrapingTimestamp\"),\n",
    "            \"parsingTimestamp\": article.get(\"parsingTimestamp\"),\n",
    "            \"sentimentTimestamp\": datetime.utcnow().isoformat(),\n",
    "            \"tickers\": tickers_output\n",
    "        }\n",
    "\n",
    "        s3.put_object(Bucket=bucket_name, Key=key, Body=json.dumps(ergebnis).encode(\"utf-8\"))\n",
    "        print(f\"‚úÖ Sentiment f√ºr {len(tickers_output)} Ticker gespeichert: {key}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Fehler bei Artikelverarbeitung: {e}\")\n",
    "\n",
    "counter = 0\n",
    "\n",
    "# Haupt-Loop mit Batching & Parallelit√§t\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    while True:\n",
    "        batch = consumer.poll(timeout_ms=1000, max_records=200)\n",
    "\n",
    "        futures = []\n",
    "        for _, messages in batch.items():\n",
    "            for msg in messages:\n",
    "                counter += 1\n",
    "                print(f'Processing message Nr. {counter}')\n",
    "                futures.append(executor.submit(process_article, msg.value))\n",
    "\n",
    "        # Warten, bis alle fertig\n",
    "        for f in futures:\n",
    "            f.result()\n",
    "\n",
    "        # Nach Erfolg Kafka-Offsets committen\n",
    "        consumer.commit()"
   ],
   "id": "42d5317c2a4d72b0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
