{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from IPython.core.interactiveshell import no_op\n",
    "\n",
    "# Folder containing the files\n",
    "folder_path = \"alphavantage_news_json_byYear\"\n",
    "\n",
    "# Get all files matching the pattern *_2024.json\n",
    "file_paths = glob.glob(os.path.join(folder_path, \"*_2024.json\"))\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # Extract ticker from filename (e.g., AAPL from AAPL_2024.json)\n",
    "    ticker = os.path.basename(file_path).split(\"_\")[0]\n",
    "\n",
    "    # Load the data\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    # Collect relevance scores\n",
    "    relevance_scores = []\n",
    "    for article in articles:\n",
    "        for sentiment in article.get(\"ticker_sentiment\", []):\n",
    "            score_str = sentiment.get(\"relevance_score\")\n",
    "            if score_str is not None:\n",
    "                try:\n",
    "                    score = float(score_str)\n",
    "                    relevance_scores.append(round(score, 2))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "    if not relevance_scores:\n",
    "        print(f\"No relevance scores found in {ticker}_2024.json\")\n",
    "        continue\n",
    "\n",
    "    # Count frequencies\n",
    "    score_counts = Counter(relevance_scores)\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(score_counts.keys(), score_counts.values(), width=0.01)\n",
    "    plt.xlabel(\"Relevance Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Histogram of Relevance Scores - {ticker} 2024\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show or save the plot\n",
    "    plt.show()\n",
    "    # Or to save: plt.savefig(f\"{ticker}_2024_relevance_hist.png\")\n",
    "\n",
    "    plt.close()\n"
   ],
   "id": "30feaffb292093a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Folder containing the files\n",
    "folder_path = \"alphavantage_news_json_byYear\"\n",
    "\n",
    "# Get all files matching the pattern *_2024.json\n",
    "file_paths = glob.glob(os.path.join(folder_path, \"*_2024.json\"))\n",
    "\n",
    "all_relevance_scores = []\n",
    "score_url_pairs = []  # (score, url) tuples\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # Extract ticker from filename (e.g., AAPL from AAPL_2024.json)\n",
    "    ticker = os.path.basename(file_path).split(\"_\")[0]\n",
    "\n",
    "    # Load the data\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    # Collect relevance scores\n",
    "    ticker_relevance_scores = []\n",
    "\n",
    "    for article in articles:\n",
    "        article_url = article.get(\"url\")\n",
    "        for sentiment in article.get(\"ticker_sentiment\", []):\n",
    "            score_str = sentiment.get(\"relevance_score\")\n",
    "            if score_str is not None:\n",
    "                try:\n",
    "                    score = float(score_str)\n",
    "                    score_rounded = round(score, 2)\n",
    "                    ticker_relevance_scores.append(score_rounded)\n",
    "                    all_relevance_scores.append(score_rounded)\n",
    "                    score_url_pairs.append((score, article_url))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "    if not ticker_relevance_scores:\n",
    "        print(f\"No relevance scores found in {ticker}_2024.json\")\n",
    "        continue\n",
    "\n",
    "# Count frequencies\n",
    "total_counts = Counter(all_relevance_scores)\n",
    "\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(total_counts.keys(), total_counts.values(), width=0.01)\n",
    "plt.xlabel(\"Relevance Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(f\"Histogram of All Relevance Scores 2024\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show or save the plot\n",
    "plt.show()\n",
    "# Or to save: plt.savefig(f\"{ticker}_2024_relevance_hist.png\")\n",
    "\n",
    "plt.close()\n"
   ],
   "id": "250b1fafa168df5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert relevance_scores to a NumPy array\n",
    "scores_array = np.array(all_relevance_scores)\n",
    "\n",
    "# Calculate five-number summary\n",
    "min_val = np.min(scores_array)\n",
    "q1 = np.percentile(scores_array, 25)\n",
    "median = np.median(scores_array)\n",
    "average = np.average(scores_array)\n",
    "q3 = np.percentile(scores_array, 75)\n",
    "max_val = np.max(scores_array)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Five-number summary for all tickers:\")\n",
    "print(f\"Min: {min_val}\")\n",
    "print(f\"Q1 : {q1}\")\n",
    "print(f\"Median: {median}\")\n",
    "print(f\"Average: {average}\")\n",
    "print(f\"Q3 : {q3}\")\n",
    "print(f\"Max: {max_val}\")\n",
    "print()"
   ],
   "id": "f9cab3e60946e95d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Scrape to .html",
   "id": "b74d3077a316f90f"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "from selenium.common import TimeoutException\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- User-Agent and headers setup ---\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0\",\n",
    "]\n",
    "\n",
    "user_agent = random.choice(USER_AGENTS)\n",
    "\n",
    "# --- Configure Chrome ---\n",
    "options = Options()\n",
    "# options.add_argument(\"--headless\")  # Uncomment to hide browser\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(f\"user-agent={user_agent}\")\n",
    "options.add_argument(\"window-size=1920,1080\")\n",
    "\n",
    "# Create output folder\n",
    "OUTPUT_FOLDER = f\"{datetime.now().strftime('%Y%m%d')}_scraping_AV_texts\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# --- Example filtered URL list (replace with your real `score_url_pairs`) ---\n",
    "# Example for context:\n",
    "# score_url_pairs = [(0.31, \"https://www.benzinga.com/article1\"), (0.42, \"https://www.investors.com/article2\")]\n",
    "filtered_urls = [url for score, url in score_url_pairs if score > 0.2]\n",
    "\n",
    "print(f\"\\nüîó URLs with relevance score > 0.2 ({len(filtered_urls)} found)\")\n",
    "\n",
    "def accept_benzinga_cookie_banner(driver):\n",
    "    try:\n",
    "        # Try finding by text (safest, language-independent)\n",
    "        accept_btn = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//a[.//span[text()='Alle akzeptieren']]\"))\n",
    "        )\n",
    "        accept_btn.click()\n",
    "        print(\"‚úÖ Clicked 'Alle akzeptieren' (by exact text)\")\n",
    "        return True\n",
    "\n",
    "    except TimeoutException:\n",
    "        # Try fallback using partial class name\n",
    "        try:\n",
    "            accept_btn = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"a.cmpboxbtnyes\"))\n",
    "            )\n",
    "            accept_btn.click()\n",
    "            print(\"‚úÖ Clicked 'Alle akzeptieren' (by CSS selector)\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(\"‚ùå Still couldn't find the Benzinga cookie banner:\", e)\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error while trying to accept Benzinga cookie banner:\", e)\n",
    "        return False\n",
    "\n",
    "# --- Main Loop ---\n",
    "for i, current_url in enumerate(filtered_urls, start=1):\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    domain = current_url.split(\"/\")[2]\n",
    "    try:\n",
    "        print(f\"\\nüåê Opening ({i}/{len(filtered_urls)}): {current_url}\")\n",
    "        driver.get(current_url)\n",
    "\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "\n",
    "        # Wait for page to load fully\n",
    "        time.sleep(2)  # adjust as needed for real-world delay\n",
    "\n",
    "        # --- Accept cookie banner if present (common fallback) ---\n",
    "        try:\n",
    "            cookie_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(., 'Accept')]\")))\n",
    "            cookie_button.click()\n",
    "            print(\"‚úÖ Cookie banner accepted\")\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è No cookie banner appeared or could not be clicked\")\n",
    "\n",
    "\n",
    "        # --- Show browser for inspection ---\n",
    "        print(f\"üîç Loaded: {current_url}\")\n",
    "        # input(\"üïµÔ∏è‚Äç‚ôÇÔ∏è Press Enter to continue to the next URL...\")\n",
    "\n",
    "        # --- Save HTML for later parsing ---\n",
    "        filename = f\"{OUTPUT_FOLDER}/{domain.replace('.', '_')}_{i}.html\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(driver.page_source)\n",
    "        print(f\"üíæ Saved HTML: {filename}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Scrape to Kafka",
   "id": "dd684e7770025bad"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-02T20:18:40.251674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "from selenium.common import TimeoutException\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from confluent_kafka import Producer, KafkaError\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import psutil\n",
    "\n",
    "\n",
    "\n",
    "# KAFKA config\n",
    "conf = {'bootstrap.servers': '172.29.16.101:9092'}\n",
    "producer = Producer(conf)\n",
    "topic = 'g3-raw-html-test'\n",
    "\n",
    "# --- User-Agent and headers setup ---\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0\",\n",
    "]\n",
    "\n",
    "user_agent = random.choice(USER_AGENTS)\n",
    "\n",
    "# --- Configure Chrome ---\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Uncomment to hide browser\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(f\"user-agent={user_agent}\")\n",
    "options.add_argument(\"window-size=1920,1080\")\n",
    "\n",
    "# Create output folder\n",
    "OUTPUT_FOLDER = f\"{datetime.now().strftime('%Y%m%d')}_scraping_AV_texts\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# --- Example filtered URL list (replace with your real `score_url_pairs`) ---\n",
    "# Example for context:\n",
    "# score_url_pairs = [(0.31, \"https://www.benzinga.com/article1\"), (0.42, \"https://www.investors.com/article2\")]\n",
    "filtered_urls = [url for score, url in score_url_pairs if score > 0.2]\n",
    "\n",
    "print(f\"\\nüîó URLs with relevance score > 0.2 ({len(filtered_urls)} found)\")\n",
    "\n",
    "def kill_orphan_chrome_processes():\n",
    "    for proc in psutil.process_iter(['name']):\n",
    "        if proc.info['name'] in ['chrome', 'chromedriver']:\n",
    "            try:\n",
    "                proc.kill()\n",
    "            except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "                pass\n",
    "\n",
    "def clean_html(raw_html):\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "\n",
    "    # Remove unwanted tags completely\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"iframe\", \"svg\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Remove comments\n",
    "    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "        comment.extract()\n",
    "\n",
    "    for tag in soup([\"header\", \"footer\", \"nav\", \"aside\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    return str(soup)\n",
    "\n",
    "\n",
    "try:\n",
    "    last = i\n",
    "    next = i + 1\n",
    "except NameError:\n",
    "    last = 0\n",
    "    next = 1\n",
    "\n",
    "# --- Main Loop ---\n",
    "for i, current_url in enumerate(filtered_urls[last:], start=next):\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "\n",
    "        domain = current_url.split(\"/\")[2]\n",
    "        print(f\"\\nüåê Opening ({i}/{len(filtered_urls)}): {current_url}\")\n",
    "        driver.get(current_url)\n",
    "\n",
    "        wait = WebDriverWait(driver,0.5)\n",
    "\n",
    "        # Wait for page to load fully\n",
    "        time.sleep(2)  # adjust as needed for real-world delay\n",
    "\n",
    "        # --- Accept cookie banner if present (common fallback) ---\n",
    "        try:\n",
    "            cookie_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(., 'Accept')]\")))\n",
    "            cookie_button.click()\n",
    "            print(\"‚úÖ Cookie banner accepted\")\n",
    "            time.sleep(0.5)\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è No cookie banner appeared or could not be clicked\")\n",
    "\n",
    "        print(f\"üîç Loaded: {current_url}\")\n",
    "\n",
    "        raw_html = driver.page_source\n",
    "        pure_html = clean_html(raw_html)\n",
    "        print(f\"Before html cleaning: {len(raw_html) / 1024:.2f} KB\")\n",
    "        print(f\"After html cleaning: {len(pure_html) / 1024:.2f} KB\")\n",
    "\n",
    "\n",
    "            # --- Prepare data payload ---\n",
    "        data = {\n",
    "            'url': current_url,\n",
    "            'html': pure_html,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        # --- Save JSON to file ---\n",
    "        filename = f\"{OUTPUT_FOLDER}/{domain.replace('.', '_')}_{i}.json\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"üíæ Saved JSON: {filename}\")\n",
    "\n",
    "        # try:\n",
    "        #     message = json.dumps(data)\n",
    "        #     print(f\"Kafka message size: {sys.getsizeof(message) / 1024:.2f} KB\")\n",
    "        #\n",
    "        #     producer.produce(topic, value=message)\n",
    "        #     producer.flush(0)\n",
    "        #     print(f'üöÄ Flushed to Kafka broker with topic \"{topic}\"')\n",
    "        #\n",
    "        # except BufferError as e:\n",
    "        #     print(f\"‚ùå Kafka BufferError: {e} ‚Äî queue is full?\")\n",
    "        # except KafkaError as e:\n",
    "        #     print(f\"‚ùå KafkaError: {e}\")\n",
    "        # except Exception as e:\n",
    "        #     print(f\"‚ùå Unexpected error: {e}\")\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        kill_orphan_chrome_processes()\n"
   ],
   "id": "18f7490f1ac3f130",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó URLs with relevance score > 0.2 (190195 found)\n",
      "\n",
      "üåê Opening (5995/190195): https://www.zacks.com/stock/news/2292751/apple-inc-aapl-is-attracting-investor-attention-here-is-what-you-should-know\n",
      "‚ö†Ô∏è No cookie banner appeared or could not be clicked\n",
      "üîç Loaded: https://www.zacks.com/stock/news/2292751/apple-inc-aapl-is-attracting-investor-attention-here-is-what-you-should-know\n",
      "Before html cleaning: 205.84 KB\n",
      "After html cleaning: 40.08 KB\n",
      "üíæ Saved JSON: 20250602_scraping_AV_texts/www_zacks_com_5995.json\n",
      "\n",
      "üåê Opening (5996/190195): https://www.zacks.com/commentary/2292453/2024-first-half-review-5-industry-stock-market-trends\n",
      "‚ö†Ô∏è No cookie banner appeared or could not be clicked\n",
      "üîç Loaded: https://www.zacks.com/commentary/2292453/2024-first-half-review-5-industry-stock-market-trends\n",
      "Before html cleaning: 215.16 KB\n",
      "After html cleaning: 44.36 KB\n",
      "üíæ Saved JSON: 20250602_scraping_AV_texts/www_zacks_com_5996.json\n",
      "\n",
      "üåê Opening (5997/190195): https://www.benzinga.com/news/24/06/39476585/chatgpt-maker-openai-to-restrict-access-to-ai-tools-in-china-amid-rising-tensions-report\n",
      "\n",
      "üåê Opening (5998/190195): https://www.benzinga.com/news/24/06/39476585/chatgpt-maker-openai-to-restrict-access-to-ai-tools-in-china-amid-rising-tensions-report\n",
      "‚ö†Ô∏è No cookie banner appeared or could not be clicked\n",
      "üîç Loaded: https://www.benzinga.com/news/24/06/39476585/chatgpt-maker-openai-to-restrict-access-to-ai-tools-in-china-amid-rising-tensions-report\n",
      "Before html cleaning: 369.91 KB\n",
      "After html cleaning: 117.58 KB\n",
      "üíæ Saved JSON: 20250602_scraping_AV_texts/www_benzinga_com_5998.json\n",
      "\n",
      "üåê Opening (5999/190195): https://www.cnn.com/2024/06/25/tech/nvidia-stock-fall-worlds-third-biggest-company/index.html\n",
      "\n",
      "üåê Opening (6000/190195): https://www.fool.com/investing/2024/06/25/who-owns-the-most-apple-stock-its-not-tim-cook/\n",
      "‚úÖ Cookie banner accepted\n",
      "üîç Loaded: https://www.fool.com/investing/2024/06/25/who-owns-the-most-apple-stock-its-not-tim-cook/\n",
      "Before html cleaning: 942.88 KB\n",
      "After html cleaning: 58.21 KB\n",
      "üíæ Saved JSON: 20250602_scraping_AV_texts/www_fool_com_6000.json\n",
      "\n",
      "üåê Opening (6001/190195): https://www.benzinga.com/news/24/06/39474703/tim-cooks-goal-to-slash-assembly-workers-by-up-to-50-faces-setback-apple-reportedly-facing-precision\n",
      "‚ö†Ô∏è No cookie banner appeared or could not be clicked\n",
      "üîç Loaded: https://www.benzinga.com/news/24/06/39474703/tim-cooks-goal-to-slash-assembly-workers-by-up-to-50-faces-setback-apple-reportedly-facing-precision\n",
      "Before html cleaning: 346.61 KB\n",
      "After html cleaning: 103.74 KB\n",
      "üíæ Saved JSON: 20250602_scraping_AV_texts/www_benzinga_com_6001.json\n",
      "\n",
      "üåê Opening (6002/190195): https://www.fool.com/investing/2024/06/25/billionaires-just-poured-about-67-billion-into-the/\n",
      "‚úÖ Cookie banner accepted\n",
      "üîç Loaded: https://www.fool.com/investing/2024/06/25/billionaires-just-poured-about-67-billion-into-the/\n",
      "Before html cleaning: 973.04 KB\n",
      "After html cleaning: 79.85 KB\n",
      "üíæ Saved JSON: 20250602_scraping_AV_texts/www_fool_com_6002.json\n",
      "\n",
      "üåê Opening (6003/190195): https://www.fool.com/investing/2024/06/25/billionaires-just-poured-about-67-billion-into-the/\n",
      "‚úÖ Cookie banner accepted\n",
      "üîç Loaded: https://www.fool.com/investing/2024/06/25/billionaires-just-poured-about-67-billion-into-the/\n",
      "Before html cleaning: 972.40 KB\n",
      "After html cleaning: 79.51 KB\n",
      "üíæ Saved JSON: 20250602_scraping_AV_texts/www_fool_com_6003.json\n",
      "\n",
      "üåê Opening (6004/190195): https://www.zacks.com/stock/news/2292615/the-zacks-analyst-blog-highlights-vistra-nvidia-apple-microsoft-and-macys\n",
      "‚ö†Ô∏è No cookie banner appeared or could not be clicked\n",
      "üîç Loaded: https://www.zacks.com/stock/news/2292615/the-zacks-analyst-blog-highlights-vistra-nvidia-apple-microsoft-and-macys\n",
      "Before html cleaning: 219.75 KB\n",
      "After html cleaning: 49.15 KB\n",
      "üíæ Saved JSON: 20250602_scraping_AV_texts/www_zacks_com_6004.json\n",
      "\n",
      "üåê Opening (6005/190195): https://www.cnn.com/2024/06/25/tech/microsoft-teams-eu-antitrust/index.html\n",
      "‚úÖ Cookie banner accepted\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "615dba423190d1d2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
