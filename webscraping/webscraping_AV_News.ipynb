{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T10:31:09.712970Z",
     "start_time": "2025-06-21T10:31:08.759090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import sys\n",
    "from selenium.common import TimeoutException\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from confluent_kafka import Producer, KafkaError\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import psutil\n",
    "\n",
    "import boto3"
   ],
   "id": "31dbcbb150c12a3e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### extract URL list from AlphaVantage downloads",
   "id": "ea83f9213784d995"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Folder containing the url-index-files\n",
    "folder_path = \"alphavantage_news_json_selection\"\n",
    "\n",
    "# Get all files matching the pattern *_2024.json\n",
    "file_paths = glob.glob(os.path.join(folder_path, \"*_2024.json\"))\n",
    "\n",
    "all_relevance_scores = []\n",
    "score_url_pairs = []  # (score, url) tuples\n",
    "domaincounter = Counter()\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # Extract ticker from filename (e.g., AAPL from AAPL_2024.json)\n",
    "    ticker = os.path.basename(file_path).split(\"_\")[0]\n",
    "\n",
    "    # Load the data\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    # Collect relevance scores\n",
    "    ticker_relevance_scores = []\n",
    "\n",
    "    for article in articles:\n",
    "        article_url = article.get(\"url\")\n",
    "        for sentiment in article.get(\"ticker_sentiment\", []):\n",
    "            score_str = sentiment.get(\"relevance_score\")\n",
    "            if score_str is not None:\n",
    "                try:\n",
    "                    score = float(score_str)\n",
    "                    score_rounded = round(score, 2)\n",
    "                    ticker_relevance_scores.append(score_rounded)\n",
    "                    all_relevance_scores.append(score_rounded)\n",
    "\n",
    "                    # Safely extract domain\n",
    "                    try:\n",
    "                        domain = article_url.split(\"/\")[2]\n",
    "                        domaincounter[domain] += 1\n",
    "                        score_url_pairs.append((score, article_url, domain))\n",
    "                    except IndexError:\n",
    "                        pass  # Skip if URL is malformed\n",
    "                except ValueError:\n",
    "                    pass  # Skip if score is not a valid float\n",
    "\n",
    "    if not ticker_relevance_scores:\n",
    "        print(f\"No relevance scores found in {ticker}_2024.json\")\n",
    "        continue\n",
    "\n",
    "# Count frequencies\n",
    "total_counts = Counter(all_relevance_scores)\n",
    "\n",
    "# Print domain stats sorted by article count (descending)\n",
    "for domain, count in domaincounter.most_common():\n",
    "    print(f\"Domain: {domain} has {count} articles\")"
   ],
   "id": "81c5e12a8450e105"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filter URLs to only include top 3 news outlets",
   "id": "bb455073ef913a42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# filtered_pairs = [\n",
    "#     (score, url, domain)\n",
    "#     for score, url, domain in score_url_pairs\n",
    "#     if score > 0.2 and any(keyword in domain for keyword in [\"benzinga\", \"fool\", \"zacks\"])\n",
    "# ]\n",
    "\n",
    "# Filter: score > 0.2 and domain contains \"benzinga\", \"fool\", or \"zacks\"\n",
    "filtered_pairs = []\n",
    "seen_urls = set()\n",
    "\n",
    "for score, url, domain in score_url_pairs:\n",
    "    if (\n",
    "            score > 0.0\n",
    "            and any(keyword in domain for keyword in [\"benzinga\", \"fool\", \"zacks\"])\n",
    "            and url not in seen_urls\n",
    "    ):\n",
    "        filtered_pairs.append((score, url, domain))\n",
    "        seen_urls.add(url)\n",
    "\n",
    "len(filtered_pairs)\n",
    "\n",
    "domaincounter = Counter()\n",
    "for _, _, domain in filtered_pairs:\n",
    "    domaincounter[domain] +=1\n",
    "\n",
    "for domain, count in domaincounter.most_common():\n",
    "    print(f\"Domain: {domain} has {count} articles\")"
   ],
   "id": "6add093986a2e1d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Get and sort previously scraped URLs",
   "id": "123a5533e5307470"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Folder containing the url-index-files\n",
    "folder_path = \"raw_unsorted\"\n",
    "destination_folder = \"raw_sorted\"\n",
    "\n",
    "# Make sure destination folder exists\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# Get all files matching the pattern *.json\n",
    "file_paths = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "\n",
    "scraped_urls = set()\n",
    "\n",
    "for file_path in file_paths:\n",
    "    filename = os.path.basename(file_path)\n",
    "    new_file_path = os.path.join(destination_folder, filename)\n",
    "\n",
    "    # Load the data\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        article = json.load(f)\n",
    "\n",
    "    article_url = article.get(\"url\")\n",
    "    domain = article_url.split(\"/\")[2]\n",
    "\n",
    "    if (\n",
    "            any(keyword in domain for keyword in [\"benzinga\", \"fool\", \"zacks\"])\n",
    "            and article_url not in scraped_urls\n",
    "    ):\n",
    "        scraped_urls.add(article_url)\n",
    "        shutil.copy(file_path, new_file_path)\n",
    "\n",
    "print(f'Anzahl bereits gescrapter Artikel: {len(scraped_urls)}')"
   ],
   "id": "957fc70ae721f6b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### generate set of outstanding URLs",
   "id": "4c6c38ae6279f9fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "outstanding_urls = set()\n",
    "\n",
    "for url in seen_urls:\n",
    "    if url not in scraped_urls:\n",
    "        outstanding_urls.add(url)\n",
    "\n",
    "print(f'Anzahl noch ausstehender Artikel: {len(outstanding_urls)}')"
   ],
   "id": "8045c0ea2680882c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Scrape outstanding URLs",
   "id": "88ffee0d156b3f35"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "# KAFKA config\n",
    "# conf = {'bootstrap.servers': '172.29.16.101:9092'}\n",
    "# producer = Producer(conf)\n",
    "# topic = 'g3-raw-html-test'\n",
    "\n",
    "USER_AGENTS = [\n",
    "    # Chrome on Windows\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\",\n",
    "\n",
    "    # Firefox on Windows\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0\",\n",
    "\n",
    "    # Edge on Windows\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0\",\n",
    "\n",
    "    # Chrome on macOS\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "\n",
    "    # Safari on macOS\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Safari/605.1.15\",\n",
    "\n",
    "    # Firefox on Linux\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:122.0) Gecko/20100101 Firefox/122.0\",\n",
    "]\n",
    "\n",
    "\n",
    "SCREEN_SIZES = [\n",
    "    \"1920,1080\",\n",
    "    \"1680,1050\",\n",
    "    \"1600,900\",\n",
    "    \"1440,900\",\n",
    "    \"1366,768\",\n",
    "    \"1280,1024\",\n",
    "    \"1024,768\"\n",
    "]\n",
    "\n",
    "\n",
    "OUTPUT_FOLDER = f\"outstanding_scraping\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "\n",
    "print(f\"\\nScraping outstanding URLs ({len(outstanding_urls)} found)\")\n",
    "\n",
    "\n",
    "def kill_orphan_chrome_processes():\n",
    "    for proc in psutil.process_iter(['name']):\n",
    "        if proc.info['name'] in ['chrome', 'chromedriver']:\n",
    "            try:\n",
    "                proc.kill()\n",
    "            except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "                pass\n",
    "\n",
    "\n",
    "def clean_html(raw_html):\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"iframe\", \"svg\", \"header\", \"footer\", \"nav\", \"aside\"]):\n",
    "        tag.decompose()\n",
    "    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "        comment.extract()\n",
    "    return str(soup)\n",
    "\n",
    "\n",
    "def get_driver():\n",
    "    user_agent = random.choice(USER_AGENTS)\n",
    "    screen_size = random.choice(SCREEN_SIZES)\n",
    "\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(f\"user-agent={user_agent}\")\n",
    "    options.add_argument(f\"window-size={screen_size}\")\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "\n",
    "def scrape_url(i, url):\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = get_driver()\n",
    "        domain = url.split(\"/\")[2]\n",
    "        print(f\"\\nüåê ({i}) Opening: {url}\")\n",
    "        driver.get(url)\n",
    "        wait = WebDriverWait(driver, 1)\n",
    "        time.sleep(3)\n",
    "\n",
    "        try:\n",
    "            cookie_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(., 'Accept')]\")))\n",
    "            cookie_button.click()\n",
    "            print(\"‚úÖ Cookie banner accepted\")\n",
    "            time.sleep(3)\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è No cookie banner appeared or could not be clicked\")\n",
    "\n",
    "        raw_html = driver.page_source\n",
    "        pure_html = clean_html(raw_html)\n",
    "\n",
    "        data = {\n",
    "            'url': url,\n",
    "            'html': pure_html,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        filename = f\"{OUTPUT_FOLDER}/{domain.replace('.', '_')}_{i}.json\"\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"üíæ Saved JSON: {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping {url}: {e}\")\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "        kill_orphan_chrome_processes()\n",
    "\n",
    "\n",
    "# --- Run Multithreaded Scraper ---\n",
    "MAX_WORKERS = 8  # Adjust depending on hardware resources\n",
    "\n",
    "try:\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = [\n",
    "            executor.submit(scrape_url, i, url)\n",
    "            for i, url in enumerate(list(outstanding_urls), start=1)\n",
    "        ]\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                future.result()  # Optional: retrieve result or catch internal exception\n",
    "            except Exception as e:\n",
    "                print(f\"Task failed: {e}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nExecution interrupted by user. Shutting down...\")\n",
    "    # Optionally: add clean-up logic here (e.g., write partially completed results to disk)"
   ],
   "id": "c318b39eef9f90d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Transfer raw scraping files to s3 server",
   "id": "4527c5d88a99a831"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-21T10:31:22.720761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "s3_endpoint =\"http://172.29.16.105:9000\"\n",
    "s3_access_key = \"bdenggroup3\"\n",
    "s3_secret_key = \"bdenggroup3\"\n",
    "bucket_name= \"bdenggroup3\"\n",
    "\n",
    "s3 = boto3.client(\"s3\",\n",
    "                  endpoint_url=s3_endpoint,\n",
    "                  aws_access_key_id=s3_access_key,\n",
    "                  aws_secret_access_key=s3_secret_key,\n",
    "                  )\n",
    "\n",
    "# local_file_path = \"raw_sorted/www_benzinga_com_136.json\"\n",
    "# remote_file_path = f'{bucket_name}/www_benzinga_com_136.json'\n",
    "\n",
    "local_folder1 = \"raw_sorted\"\n",
    "local_folder2 = \"outstanding_scraping\"\n",
    "\n",
    "file_paths1 = glob.glob(os.path.join(local_folder1, \"*.json\"))\n",
    "file_paths2 = glob.glob(os.path.join(local_folder2, \"*.json\"))\n",
    "\n",
    "file_paths = file_paths1 + file_paths2\n",
    "\n",
    "i = 0\n",
    "\n",
    "for local_file_path in file_paths:\n",
    "    remote_file_path = f'raw/scrape_raw_{i}.json'\n",
    "    s3.upload_file(local_file_path, bucket_name, remote_file_path)\n",
    "    i+=1\n",
    "    if (i%100 ==0):\n",
    "        print (f'Checkpoint: {i} of {len(file_paths)} uploaded')\n",
    "\n"
   ],
   "id": "fd0ad3f19f4a4679",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: 100 of 43526 uploaded\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
