{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "FOOL",
   "id": "d801f48e5dfce1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T18:34:25.571145Z",
     "start_time": "2025-06-24T18:34:25.355867Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load your HTML string or file\n",
    "with open(\"fool.htm\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html = f.read()\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Find the main article container\n",
    "article_body = soup.find(\"div\", class_=\"article-body\")\n",
    "\n",
    "# Extract all text from <p> and <h2> tags inside it\n",
    "if article_body:\n",
    "    paragraphs = article_body.find_all(['p', 'h2'])\n",
    "    article_text = \"\\n\\n\".join(tag.get_text(strip=True) for tag in paragraphs)\n",
    "    print(article_text)\n",
    "else:\n",
    "    print(\"No article-body div found.\")\n"
   ],
   "id": "a1147743d0c7fe19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividend stocks are a great way to generate income from your investments. However, mostdividend stocksonly provide payouts once a quarter. Some pay only semi-annually, and others pay their dividend just once per year. That can result in a very lumpy stream of income.\n",
      "\n",
      "If you want consistent monthly income, just a handful of companies will pay out their dividend every month. That means finding one of those companies trading at a fair price is an absolute gem for income investors. Two companies can meet those narrow criteria and are worth a closer look for investors interested in monthly income:Realty Income(O0.52%)andSL Green Realty(SLG-0.18%).\n",
      "\n",
      "Image source: Getty Images.\n",
      "\n",
      "1. Realty Income\n",
      "\n",
      "Realty Income prides itself on paying a monthly dividend -- even going so far as to itself \"the monthly dividend company.\" The real estate investment trust, orREIT, has managed to raise that monthly dividend 123 times since going public in 1994. Its most recent dividend increase brought the annualized dividend to $3.072 per share.\n",
      "\n",
      "Realty Income has built a portfolio of properties and filled them with tenants that have been able to weather numerous economic downturns. All of its top 20 tenants fall into at least one of the following categories: Non-discretionary, low price point, service-oriented, or non-retail. That makes them far more resilient than something like high-end luxury retailers, which may see a downturn in sales amid arecession.\n",
      "\n",
      "As a result, Realty Income's occupancy rate remains consistently high. It managed to fill 98.8% of its units in the third quarter. Its long-term leases also ensure it'll maintain that high occupancy rate. The average lease still has about 10 years left.\n",
      "\n",
      "The downside of Realty Income's strategy is that it means growth is slow and steady. Rents increase just a percentage point or two per year. As such, its main form of growth is through acquisitions. Its most recent one,Spirit Realty, will cost more than $9 billion, but add 2,000 new properties with a similar tenant profile to Realty Income's existing clients.\n",
      "\n",
      "Management expects that Spirit Realty acquisition to help it grow adjustedfunds from operations(AFFO) by4% to 5% next year. That puts its share price around 13.7 times next year's AFFO outlook. That's a more than fair price to pay for a company with consistently growing profits and free cash flow, and with the dividend track record of Realty Income.\n",
      "\n",
      "2. SL Green\n",
      "\n",
      "SL Green is also a REIT and it is the largest owner of office space in Manhattan. It has interest in 59 different buildings totaling 32.5 million square feet of space.\n",
      "\n",
      "That was a great position to be in pre-pandemic. But the rise of remote work amid the COVID-19 pandemic and the subsequent hike in interest rates all worked against SL Green over the last few years. Occupancy rates fell from 96% at the end of 2019 to below 90% by mid-year 2023.\n",
      "\n",
      "Management worked to reposition itself over the last few years. It sold off several non-core properties, refocusing on increasing tenancy. It's had to accept lower rates, but those rates remain profitable. It's finally starting to see the fruits of its labor, as occupancy rates crept up by a tenth of a percentage point in the third quarter.\n",
      "\n",
      "Unfortunately, those moves haven't been enough to spare SL Green's dividend. Management cut the dividend for the second year in a row in December. It now pays an annual dividend of $3, $0.25 per month.\n",
      "\n",
      "The good news for investors is that the worst is likely over for the company. As mentioned, occupancy rates are starting to improve. Asset sales are helping shore up cash and pay down debt, including the recent sale of 625 Madison Avenue. And expected interest rate cuts in 2024 should help lower its cost of debt and improve its acquisition potential.\n",
      "\n",
      "Management guided for 2024 funds from operations between $4.90 and $5.20 per share. The shares trade for around 9 times that amount, which is a big jump in valuation from where it was just a few months ago. But with the improved interest rate outlook, it could be worth picking up a small position in the riskier dividend stock.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T18:34:29.265470Z",
     "start_time": "2025-06-24T18:34:29.260594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_article_text(html_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts readable article text from a Motley Fool-style HTML file.\n",
    "\n",
    "    Args:\n",
    "        html_path (str): Path to the HTML file.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned article text with paragraphs and section headings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(html_path, 'r', encoding='utf-8') as f:\n",
    "            html = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        article_body = soup.find(\"div\", class_=\"article-body\")\n",
    "\n",
    "        if article_body:\n",
    "            tags = article_body.find_all(['p', 'h2'])\n",
    "            text = \"\\n\\n\".join(tag.get_text(strip=True) for tag in tags)\n",
    "            return text\n",
    "        else:\n",
    "            return \"[!] No <div class='article-body'> found in the HTML.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"[!] Error reading or parsing file: {e}\"\n"
   ],
   "id": "71fc9e619b118a2f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T18:34:32.425980Z",
     "start_time": "2025-06-24T18:34:32.361269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = extract_article_text(\"fool.htm\")\n",
    "print(text)\n"
   ],
   "id": "7dca2b8a5aa0398c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividend stocks are a great way to generate income from your investments. However, mostdividend stocksonly provide payouts once a quarter. Some pay only semi-annually, and others pay their dividend just once per year. That can result in a very lumpy stream of income.\n",
      "\n",
      "If you want consistent monthly income, just a handful of companies will pay out their dividend every month. That means finding one of those companies trading at a fair price is an absolute gem for income investors. Two companies can meet those narrow criteria and are worth a closer look for investors interested in monthly income:Realty Income(O0.52%)andSL Green Realty(SLG-0.18%).\n",
      "\n",
      "Image source: Getty Images.\n",
      "\n",
      "1. Realty Income\n",
      "\n",
      "Realty Income prides itself on paying a monthly dividend -- even going so far as to itself \"the monthly dividend company.\" The real estate investment trust, orREIT, has managed to raise that monthly dividend 123 times since going public in 1994. Its most recent dividend increase brought the annualized dividend to $3.072 per share.\n",
      "\n",
      "Realty Income has built a portfolio of properties and filled them with tenants that have been able to weather numerous economic downturns. All of its top 20 tenants fall into at least one of the following categories: Non-discretionary, low price point, service-oriented, or non-retail. That makes them far more resilient than something like high-end luxury retailers, which may see a downturn in sales amid arecession.\n",
      "\n",
      "As a result, Realty Income's occupancy rate remains consistently high. It managed to fill 98.8% of its units in the third quarter. Its long-term leases also ensure it'll maintain that high occupancy rate. The average lease still has about 10 years left.\n",
      "\n",
      "The downside of Realty Income's strategy is that it means growth is slow and steady. Rents increase just a percentage point or two per year. As such, its main form of growth is through acquisitions. Its most recent one,Spirit Realty, will cost more than $9 billion, but add 2,000 new properties with a similar tenant profile to Realty Income's existing clients.\n",
      "\n",
      "Management expects that Spirit Realty acquisition to help it grow adjustedfunds from operations(AFFO) by4% to 5% next year. That puts its share price around 13.7 times next year's AFFO outlook. That's a more than fair price to pay for a company with consistently growing profits and free cash flow, and with the dividend track record of Realty Income.\n",
      "\n",
      "2. SL Green\n",
      "\n",
      "SL Green is also a REIT and it is the largest owner of office space in Manhattan. It has interest in 59 different buildings totaling 32.5 million square feet of space.\n",
      "\n",
      "That was a great position to be in pre-pandemic. But the rise of remote work amid the COVID-19 pandemic and the subsequent hike in interest rates all worked against SL Green over the last few years. Occupancy rates fell from 96% at the end of 2019 to below 90% by mid-year 2023.\n",
      "\n",
      "Management worked to reposition itself over the last few years. It sold off several non-core properties, refocusing on increasing tenancy. It's had to accept lower rates, but those rates remain profitable. It's finally starting to see the fruits of its labor, as occupancy rates crept up by a tenth of a percentage point in the third quarter.\n",
      "\n",
      "Unfortunately, those moves haven't been enough to spare SL Green's dividend. Management cut the dividend for the second year in a row in December. It now pays an annual dividend of $3, $0.25 per month.\n",
      "\n",
      "The good news for investors is that the worst is likely over for the company. As mentioned, occupancy rates are starting to improve. Asset sales are helping shore up cash and pay down debt, including the recent sale of 625 Madison Avenue. And expected interest rate cuts in 2024 should help lower its cost of debt and improve its acquisition potential.\n",
      "\n",
      "Management guided for 2024 funds from operations between $4.90 and $5.20 per share. The shares trade for around 9 times that amount, which is a big jump in valuation from where it was just a few months ago. But with the improved interest rate outlook, it could be worth picking up a small position in the riskier dividend stock.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T18:35:08.290012Z",
     "start_time": "2025-06-24T18:35:08.284630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_article_text_from_json(json_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts article text from a JSON file that contains raw HTML under the 'html' key.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned article text or an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        html = data.get('html')\n",
    "        if not html:\n",
    "            return \"[!] No 'html' key found in the JSON.\"\n",
    "\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        article_body = soup.find(\"div\", class_=\"article-body\")\n",
    "\n",
    "        if article_body:\n",
    "            tags = article_body.find_all(['p', 'h2'])\n",
    "            text = \"\\n\\n\".join(tag.get_text(strip=True) for tag in tags)\n",
    "            return text\n",
    "        else:\n",
    "            return \"[!] No <div class='article-body'> found in the HTML.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"[!] Error processing JSON file: {e}\"\n"
   ],
   "id": "843c1caca77b9585",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T18:35:09.986613Z",
     "start_time": "2025-06-24T18:35:09.981680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "article_text = extract_article_text_from_json(\"data/www_fool_com_54277.json\")\n",
    "print(article_text)\n"
   ],
   "id": "aa9f80af5735c49f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] Error processing JSON file: [Errno 2] No such file or directory: 'data/www_fool_com_54277.json'\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Fool Timestamp\n",
   "id": "e376b87cd2b48900"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T18:35:30.134314Z",
     "start_time": "2025-06-24T18:35:30.130917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime, timezone\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_fool_publish_date(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    meta_tag = soup.find(\"meta\", attrs={\"property\": \"article:published_time\"})\n",
    "    if meta_tag and meta_tag.get(\"content\"):\n",
    "        dt = datetime.fromisoformat(meta_tag[\"content\"])\n",
    "        dt_utc = dt.astimezone(timezone.utc)\n",
    "        return dt_utc.replace(tzinfo=None).isoformat(timespec=\"microseconds\")\n",
    "\n",
    "    return \"\"\n"
   ],
   "id": "5b481075e01d1b8f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T18:35:32.569513Z",
     "start_time": "2025-06-24T18:35:32.237241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/scrape_raw_9999.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "html = data.get(\"html\", \"\")\n",
    "pubDate = extract_fool_publish_date(html)\n",
    "print(\"Publishing Date:\", pubDate)\n",
    "\n"
   ],
   "id": "d389b5689bcc4199",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/scrape_raw_9999.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mjson\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdata/scrape_raw_9999.json\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mutf-8\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m      4\u001B[39m     data = json.load(f)\n\u001B[32m      6\u001B[39m html = data.get(\u001B[33m\"\u001B[39m\u001B[33mhtml\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\DataspellProjects\\fhtw-bdeng-project\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:326\u001B[39m, in \u001B[36m_modified_open\u001B[39m\u001B[34m(file, *args, **kwargs)\u001B[39m\n\u001B[32m    319\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m}:\n\u001B[32m    320\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    321\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mIPython won\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m by default \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    322\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    323\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33myou can use builtins\u001B[39m\u001B[33m'\u001B[39m\u001B[33m open.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    324\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m326\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'data/scrape_raw_9999.json'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BENZINGA",
   "id": "89f89dcc871dde13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_benzinga_article_text(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts main article text from Benzinga-style HTML, flattens for NLP.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Main content container for the article body\n",
    "    body_div = soup.find(\"div\", {\"id\": \"article-body\"})\n",
    "    if not body_div:\n",
    "        return \"\"\n",
    "\n",
    "    # Extract paragraphs and headings\n",
    "    tags = body_div.find_all([\"p\", \"h2\", \"li\"])\n",
    "    text_parts = [tag.get_text(strip=True) for tag in tags]\n",
    "\n",
    "    # Join and clean\n",
    "    raw_text = \" \".join(text_parts)\n",
    "    clean_text = re.sub(r'\\s+', ' ', raw_text).strip()\n",
    "\n",
    "    return clean_text\n"
   ],
   "id": "e293cc0ba3b9cdff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open(\"benzinga.htm\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "text = extract_benzinga_article_text(html_content)\n",
    "print(text[:5000])  # Preview first 500 chars\n"
   ],
   "id": "1c10af9ed02ad848"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T12:08:08.826952Z",
     "start_time": "2025-06-21T12:08:08.816409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_benzinga_publish_date(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the publishing date from Benzinga HTML assuming consistent format.\n",
    "    Example format: 'June 21, 2025 3:23 AM' → ISO 8601.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    date_span = soup.find(\"span\", class_=\"article-date\")\n",
    "    if not date_span:\n",
    "        return \"\"\n",
    "\n",
    "    raw_date = date_span.get_text(strip=True)\n",
    "\n",
    "    # Parse with fixed format\n",
    "    dt = datetime.strptime(raw_date, \"%B %d, %Y %I:%M %p\")\n",
    "    return dt.isoformat()\n"
   ],
   "id": "37a56444ff72178d",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-21T12:10:18.698101Z",
     "start_time": "2025-06-21T12:10:18.457952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"benzinga.htm\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "text = extract_benzinga_publish_date(html_content)\n",
    "print(text)  # Prev"
   ],
   "id": "d11fe5ca0315814a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-01T12:00:00\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_zacks_article_text(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts main article text from Zacks-style HTML and removes promotional ad blocks.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    body_div = soup.find(\"div\", {\"id\": \"comtext\"})\n",
    "    if not body_div:\n",
    "        return \"\"\n",
    "\n",
    "    # Extract paragraph, heading, and list elements\n",
    "    tags = body_div.find_all([\"p\", \"h2\", \"li\"])\n",
    "    text_parts = [tag.get_text(strip=True) for tag in tags]\n",
    "\n",
    "    # Combine into a single string\n",
    "    raw_text = \" \".join(text_parts)\n",
    "    clean_text = re.sub(r'\\s+', ' ', raw_text).strip()\n",
    "\n",
    "    # Remove text between known ad break markers\n",
    "    ad_pattern = r'-{5,}.*?-{5,}'\n",
    "    clean_text = re.sub(ad_pattern, '', clean_text)\n",
    "\n",
    "    return clean_text.strip()\n",
    "\n",
    "\n"
   ],
   "id": "9a412c16fb0ec0bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test locally with saved file\n",
    "with open(\"zacks.htm\", \"r\", encoding=\"utf-8\") as f:\n",
    "    html_content = f.read()\n",
    "\n",
    "text = extract_zacks_article_text(html_content)\n",
    "print(text)  # Preview first 5000 chars"
   ],
   "id": "d7f19d471ca7598d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# FINAL\n",
   "id": "c4b899f8c032f276"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Imports",
   "id": "69acc049c3a02a3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n"
   ],
   "id": "4d6abaa84115d8e4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## HTML Extraction Helper\n",
    "fool"
   ],
   "id": "a1f3d598dd18bce6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_fool_article_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    article_body = soup.find(\"div\", class_=\"article-body\")\n",
    "    if article_body:\n",
    "        tags = article_body.find_all(['p', 'h2'])\n",
    "        flat_text = ' '.join(tag.get_text(strip=True) for tag in tags)\n",
    "        return re.sub(r'\\s+', ' ', flat_text).strip()\n",
    "    return \"\"\n"
   ],
   "id": "960d1dfbddd44145"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "benzinga",
   "id": "c43d41f03be750bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def extract_benzinga_article_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    article_body = soup.find(\"div\", id=\"article-body\")\n",
    "    if article_body:\n",
    "        tags = article_body.find_all(['p', 'h2', 'li'])\n",
    "        flat_text = ' '.join(tag.get_text(strip=True) for tag in tags)\n",
    "        return re.sub(r'\\s+', ' ', flat_text).strip()\n",
    "    return \"\""
   ],
   "id": "50f7f1084d7f2096"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "dynamic dispatcher\n",
   "id": "b12c6f531acaa27b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_article_text_by_source(url: str, html: str) -> str:\n",
    "    if \"fool.com\" in url:\n",
    "        return extract_fool_article_text(html)\n",
    "    elif \"benzinga.com\" in url:\n",
    "        return extract_benzinga_article_text(html)\n",
    "    else:\n",
    "        return \"\"\n"
   ],
   "id": "a9cb2b4e95114961"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "full file processing",
   "id": "4fa01966299ed526"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def process_json_file(input_path: str) -> str:\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        html = data.get(\"html\", \"\")\n",
    "        url = data.get(\"url\", \"\")\n",
    "        scraping_timestamp = data.get(\"timestamp\", \"\")\n",
    "\n",
    "        article_text = extract_article_text_by_source(url, html)\n",
    "        parsed_data = {\n",
    "            \"url\": url,\n",
    "            \"scrapingTimestamp\": scraping_timestamp,\n",
    "            \"parsingTimestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"articleText\": article_text\n",
    "        }\n",
    "\n",
    "        # Create the new filename\n",
    "        dirname, filename = os.path.split(input_path)\n",
    "        cleaned_filename = filename.replace(\"scrape_raw_\", \"scrape_cleaned_\")\n",
    "        output_path = os.path.join(dirname, cleaned_filename)\n",
    "\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(parsed_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        return output_path\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error processing {input_path}: {e}\"\n",
    "\n"
   ],
   "id": "c73bfa0c5bbce548"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Batch Processing",
   "id": "299acd5a6042c453"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def process_all_json_files(folder_path: str):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".json\") and not filename.endswith(\"_parsed.json\"):\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            output = process_json_file(full_path)\n",
    "            print(f\"✅ Processed: {filename} → {output}\")\n"
   ],
   "id": "86748cc2225c8992"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run Function",
   "id": "f83603b6127373d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# TODO: Adjust path\n",
    "process_all_json_files(\"./data\")\n"
   ],
   "id": "e81aa4d2b7dd3636"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
