{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Download Historical Stock Data",
   "id": "c1e1a7763b3ff99f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Prepare Constants",
   "id": "b3036b868c7998e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T19:12:03.496236Z",
     "start_time": "2025-06-24T19:12:03.491796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "TICKER_LIST = [\n",
    "    \"AAPL\", \"TSLA\", \"GOOG\", \"NVDA\", \"O\", \"BA\"\n",
    "]\n",
    "\n",
    "start_date = '2023-12-31'\n",
    "end_date = '2025-01-01'\n",
    "\n",
    "CSV_DIR = \"./stock_csv_data\""
   ],
   "id": "c8e02815c875aa82",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Download Stock Data and save to csv\n",
    "One .csv per Ticker"
   ],
   "id": "85d0b382e424ba7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T19:12:16.574898Z",
     "start_time": "2025-06-24T19:12:03.501116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from yahooquery import Ticker\n",
    "\n",
    "os.makedirs(CSV_DIR, exist_ok=True)\n",
    "\n",
    "for TICKER in TICKER_LIST:\n",
    "    print(f\"ğŸ“¥ Downloading {TICKER}\")\n",
    "\n",
    "    # Fetch data\n",
    "    ticker = Ticker(TICKER)\n",
    "    data = ticker.history(start=start_date, end=end_date, interval=\"1d\")\n",
    "    print(f\"âœ… {TICKER} data fetched.\")\n",
    "\n",
    "    # Skip if data is empty\n",
    "    if data.empty:\n",
    "        print(f\"âš ï¸ No data for {TICKER}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Reset index to expose \"date\" as a column\n",
    "    data = data.reset_index()\n",
    "\n",
    "    # Check for \"date\" column existence\n",
    "    if \"date\" not in data.columns:\n",
    "        print(f\"âš ï¸ 'date' column missing for {TICKER}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Convert and localize datetime\n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"], errors='coerce')\n",
    "    data = data.dropna(subset=[\"date\"])\n",
    "    if data[\"date\"].dt.tz is None:\n",
    "        data[\"date\"] = data[\"date\"].dt.tz_localize(\"UTC\")\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_path = os.path.join(CSV_DIR, f\"{TICKER}.csv\")\n",
    "    data.to_csv(csv_path, index=False)\n",
    "    print(f\"ğŸ“ CSV gespeichert: {csv_path}\")\n"
   ],
   "id": "9ca5451c40f4af40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Downloading AAPL\n",
      "âœ… AAPL data fetched.\n",
      "ğŸ“ CSV gespeichert: ./stock_csv_data\\AAPL.csv\n",
      "ğŸ“¥ Downloading TSLA\n",
      "âœ… TSLA data fetched.\n",
      "ğŸ“ CSV gespeichert: ./stock_csv_data\\TSLA.csv\n",
      "ğŸ“¥ Downloading GOOG\n",
      "âœ… GOOG data fetched.\n",
      "ğŸ“ CSV gespeichert: ./stock_csv_data\\GOOG.csv\n",
      "ğŸ“¥ Downloading NVDA\n",
      "âœ… NVDA data fetched.\n",
      "ğŸ“ CSV gespeichert: ./stock_csv_data\\NVDA.csv\n",
      "ğŸ“¥ Downloading O\n",
      "âœ… O data fetched.\n",
      "ğŸ“ CSV gespeichert: ./stock_csv_data\\O.csv\n",
      "ğŸ“¥ Downloading BA\n",
      "âœ… BA data fetched.\n",
      "ğŸ“ CSV gespeichert: ./stock_csv_data\\BA.csv\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Index Stock data from .csv to InfluxDB\n",
   "id": "811f883ac86adaad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T19:12:17.156789Z",
     "start_time": "2025-06-24T19:12:16.736397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from influxdb_client import InfluxDBClient, Point, WriteOptions\n",
    "\n",
    "# ğŸ”§ InfluxDB-Konfiguration\n",
    "INFLUXDB_URL = \"http://localhost:10896\"\n",
    "INFLUXDB_TOKEN = \"14iJvsBJKp37nLXjIZvE4RbAoEO2dNs1k0GvCbKuJUnF_ub4pSWWw80O739jabLPMD-XBzA72WSX9f-4FuDBQ==\"\n",
    "INFLUXDB_ORG = \"bdinf-org\"\n",
    "INFLUXDB_BUCKET = \"bdinf-bucket\"\n",
    "\n",
    "# ğŸ“ Pfad zu CSV-Dateien (musst du ggf. anpassen)\n",
    "CSV_DIR = \"./stock_csv_data\"  # <== SETZE HIER DEINEN CSV-PFAD EIN\n",
    "\n",
    "# ğŸ”Œ InfluxDB-Client initialisieren\n",
    "client = InfluxDBClient(\n",
    "    url=INFLUXDB_URL,\n",
    "    token=INFLUXDB_TOKEN,\n",
    "    org=INFLUXDB_ORG\n",
    ")\n",
    "\n",
    "write_api = client.write_api(write_options=WriteOptions(\n",
    "    batch_size=5000,\n",
    "    flush_interval=5_000,\n",
    "    jitter_interval=1_000,\n",
    "    retry_interval=5_000,\n",
    "    max_retries=5,\n",
    "    max_retry_delay=30_000,\n",
    "    exponential_base=2\n",
    "))\n",
    "\n",
    "# ğŸ“¤ CSV-Dateien verarbeiten und hochladen\n",
    "for ticker_file in os.listdir(CSV_DIR):\n",
    "    if not ticker_file.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    TICKER = os.path.splitext(ticker_file)[0]\n",
    "    print(f\"ğŸ“¤ Uploading {TICKER} to InfluxDB\")\n",
    "\n",
    "    data_path = os.path.join(CSV_DIR, ticker_file)\n",
    "    data = pd.read_csv(data_path)\n",
    "\n",
    "    if data.empty or \"date\" not in data.columns:\n",
    "        print(f\"âš ï¸ Skipping {TICKER}, invalid or empty data\")\n",
    "        continue\n",
    "\n",
    "    data[\"date\"] = pd.to_datetime(data[\"date\"], errors='coerce')\n",
    "    data = data.dropna(subset=[\"date\"])\n",
    "    if data.empty:\n",
    "        print(f\"âš ï¸ Skipping {TICKER}, no valid dates after cleaning\")\n",
    "        continue\n",
    "\n",
    "    points = []\n",
    "    for _, row in data.iterrows():\n",
    "        timestamp = row[\"date\"]\n",
    "        if pd.isna(timestamp):\n",
    "            continue\n",
    "\n",
    "        if timestamp.tzinfo is None:\n",
    "            timestamp = timestamp.tz_localize(\"UTC\")\n",
    "        else:\n",
    "            timestamp = timestamp.astimezone(datetime.timezone.utc)\n",
    "\n",
    "        try:\n",
    "            point = (\n",
    "            Point(\"hist_stock_data\")\n",
    "            .tag(\"ticker\", row[\"symbol\"])  # Aus CSV direkt\n",
    "            .field(\"open\", float(row[\"open\"]))\n",
    "            .field(\"high\", float(row[\"high\"]))\n",
    "            .field(\"low\", float(row[\"low\"]))\n",
    "            .field(\"close\", float(row[\"close\"]))\n",
    "            .field(\"adjclose\", float(row[\"adjclose\"]))  # kleiner Buchstabe \"c\"\n",
    "            .field(\"volume\", int(row[\"volume\"]))\n",
    "            .time(timestamp)\n",
    "            )\n",
    "            points.append(point)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error in {TICKER} row: {e}\")\n",
    "            continue\n",
    "\n",
    "    if points:\n",
    "        write_api.write(bucket=INFLUXDB_BUCKET, org=INFLUXDB_ORG, record=points)\n",
    "        print(f\"âœ… {TICKER} data written to InfluxDB\\n\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ No valid data points for {TICKER}\\n\")\n",
    "\n",
    "# ğŸ”’ Verbindung schlieÃŸen\n",
    "client.close()\n",
    "print(\"Import abgeschlossen. Verbindung geschlossen.\")"
   ],
   "id": "b9fa5806c072ba64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¤ Uploading AAPL to InfluxDB\n",
      "âœ… AAPL data written to InfluxDB\n",
      "\n",
      "ğŸ“¤ Uploading BA to InfluxDB\n",
      "âœ… BA data written to InfluxDB\n",
      "\n",
      "ğŸ“¤ Uploading GOOG to InfluxDB\n",
      "âœ… GOOG data written to InfluxDB\n",
      "\n",
      "ğŸ“¤ Uploading NVDA to InfluxDB\n",
      "âœ… NVDA data written to InfluxDB\n",
      "\n",
      "ğŸ“¤ Uploading O to InfluxDB\n",
      "âœ… O data written to InfluxDB\n",
      "\n",
      "ğŸ“¤ Uploading TSLA to InfluxDB\n",
      "âœ… TSLA data written to InfluxDB\n",
      "\n",
      "Import abgeschlossen. Verbindung geschlossen.\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
