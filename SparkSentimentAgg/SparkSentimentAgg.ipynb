{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Spark Sentiment Daily Aggregation",
   "id": "c9ea085a615c37b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:13:20.571073Z",
     "start_time": "2025-06-23T19:13:20.566839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import explode, col, to_date, avg, sum as _sum, count\n",
    "import boto3\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from influxdb_client import InfluxDBClient, Point, WritePrecision\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# üìç InfluxDB-Konfiguration\n",
    "INFLUXDB_URL = \"http://localhost:10896\"\n",
    "INFLUXDB_TOKEN = \"14iJvsBJKp37nLXjIZvE4RbAoEO2dNs1k0GvCbKuJUnF_ub4pSWWw80O739jabLPMD-XBzA72WSX9f-4FuDBQ==\"\n",
    "INFLUXDB_ORG = \"bdinf-org\"\n",
    "INFLUXDB_BUCKET = \"bdinf-bucket\"\n",
    "\n",
    "s3_endpoint_url=\"http://172.29.16.105:9000\"\n",
    "s3_access_key_id=\"bdenggroup3\"\n",
    "s3_secret_access_key=\"bdenggroup3\"\n",
    "bucket_name = \"bdenggroup3\"\n",
    "s3_prefix = \"sentiment\"\n",
    "\n",
    "spark_master_url = \"spark://localhost:7077\""
   ],
   "id": "ac4c2d0ef8626bcb",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:09:08.958575Z",
     "start_time": "2025-06-23T19:09:08.833612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# üß† SparkSession mit MinIO S3-Kompatibilit√§t\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sentiment Aggregator\") \\\n",
    "    .master(spark_master_url) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ],
   "id": "4779e00686f52860",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:09:10.562363Z",
     "start_time": "2025-06-23T19:09:10.551622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Set up boto3 client ===\n",
    "s3 = boto3.client(\"s3\",\n",
    "                  endpoint_url=s3_endpoint_url,\n",
    "                  aws_access_key_id=s3_access_key_id,\n",
    "                  aws_secret_access_key=s3_secret_access_key\n",
    "                  )\n",
    "\n",
    "# === List all JSON files under the prefix ===\n",
    "keys = []\n",
    "continuation_token = None"
   ],
   "id": "a27533a95bad912a",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:09:14.685843Z",
     "start_time": "2025-06-23T19:09:11.781287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "while True:\n",
    "    if continuation_token:\n",
    "        response = s3.list_objects_v2(\n",
    "            Bucket=bucket_name, Prefix=s3_prefix, ContinuationToken=continuation_token)\n",
    "    else:\n",
    "        response = s3.list_objects_v2(\n",
    "            Bucket=bucket_name, Prefix=s3_prefix)\n",
    "\n",
    "    for obj in response.get(\"Contents\", []):\n",
    "        key = obj[\"Key\"]\n",
    "        if key.endswith(\".json\"):\n",
    "            keys.append(key)\n",
    "\n",
    "    if response.get(\"IsTruncated\"):  # more data available\n",
    "        continuation_token = response.get(\"NextContinuationToken\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(f\"Found {len(keys)} JSON files\")"
   ],
   "id": "dab5b41da6845c2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20905 JSON files\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# === Read and parse JSON files ===\n",
    "key_counter = 0\n",
    "rows = []\n",
    "\n",
    "for key in keys:\n",
    "    key_counter += 1\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "        data = json.load(obj[\"Body\"])\n",
    "        article_timestamp = data.get(\"articleTimestamp\")\n",
    "        url = data.get(\"url\", \"\")\n",
    "        if not article_timestamp:\n",
    "            continue\n",
    "        article_date = article_timestamp[:10]  # 'YYYY-MM-DD'\n",
    "        domain = urlparse(url).netloc.lower()\n",
    "\n",
    "        tickers = data.get(\"tickers\", [])\n",
    "        for ticker in tickers:\n",
    "            sentiment = ticker.get(\"sentiment_score\")\n",
    "            relevance = ticker.get(\"relevance_score\")\n",
    "            symbol = ticker.get(\"ticker\")\n",
    "            if sentiment is not None and relevance is not None and symbol:\n",
    "                weighted_sentiment = sentiment * relevance\n",
    "                rows.append(Row(\n",
    "                    article_date=article_date,\n",
    "                    ticker=symbol,\n",
    "                    sentiment=sentiment,\n",
    "                    relevance=relevance,\n",
    "                    weighted_sentiment=weighted_sentiment,\n",
    "                    domain=domain\n",
    "                ))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {key}: {e}\")\n",
    "\n",
    "    if key_counter % 100 == 0:\n",
    "        print(f\"Processed {key_counter} JSON files\")\n",
    "\n",
    "    if key_counter % 100000 == 0:\n",
    "        print(f\"Cancelled after {key_counter} JSON files\")\n",
    "        break\n",
    "\n",
    "# Optional final update\n",
    "print(f\"Finished processing {key_counter} JSON files total.\")\n"
   ],
   "id": "ad777417fb78da22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:09:26.374841Z",
     "start_time": "2025-06-23T19:09:20.584598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create Spark DataFrame from rows\n",
    "df = spark.createDataFrame(rows)\n",
    "\n",
    "# Filter to specific date and select unique domains\n",
    "target_date = \"2025-06-21\"\n",
    "\n",
    "domains_df = df.filter(col(\"article_date\") != target_date).select(\"domain\").distinct()\n",
    "\n",
    "# Show the result\n",
    "domains_df.show(truncate=False)\n",
    "\n",
    "# Optional: Save to file\n",
    "domains_df.toPandas().to_csv(f\"domains_{target_date}.csv\", index=False)\n"
   ],
   "id": "f03c6826b5412ba1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|domain          |\n",
      "+----------------+\n",
      "|www.benzinga.com|\n",
      "|www.fool.com    |\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:09:30.807681Z",
     "start_time": "2025-06-23T19:09:28.921869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# === Create DataFrame and compute relevance-weighted average ===\n",
    "df = spark.createDataFrame(rows)\n",
    "\n",
    "agg_df = df.groupBy(\"article_date\", \"ticker\").agg(\n",
    "    _sum(\"weighted_sentiment\").alias(\"total_weighted_sentiment\"),\n",
    "    count(\"*\").alias(\"article_count\")\n",
    ")\n",
    "\n",
    "# Step 2: Compute daily average sentiment per ticker\n",
    "agg_df = agg_df.withColumn(\n",
    "    \"daily_sentiment\",\n",
    "    col(\"total_weighted_sentiment\") / col(\"article_count\")\n",
    ")\n",
    "\n",
    "agg_df.orderBy(\"article_date\", \"ticker\").show(truncate=False)"
   ],
   "id": "bba709e5a0327ee0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+------------------------+-------------+---------------------+\n",
      "|article_date|ticker|total_weighted_sentiment|article_count|daily_sentiment      |\n",
      "+------------+------+------------------------+-------------+---------------------+\n",
      "|2022-10-14  |TSLA  |0.0                     |1            |0.0                  |\n",
      "|2024-01-01  |AAPL  |0.07176399999999998     |4            |0.017940999999999995 |\n",
      "|2024-01-01  |GOOG  |0.481741                |5            |0.0963482            |\n",
      "|2024-01-01  |NVDA  |0.777533                |6            |0.12958883333333335  |\n",
      "|2024-01-01  |O     |0.112407                |1            |0.112407             |\n",
      "|2024-01-01  |TSLA  |0.7458720000000001      |9            |0.08287466666666668  |\n",
      "|2024-01-02  |AAPL  |-1.214044               |14           |-0.08671742857142857 |\n",
      "|2024-01-02  |BA    |-0.6                    |1            |-0.6                 |\n",
      "|2024-01-02  |GOOG  |1.018313                |7            |0.14547328571428572  |\n",
      "|2024-01-02  |NVDA  |2.396275                |15           |0.15975166666666668  |\n",
      "|2024-01-02  |TSLA  |-0.41278100000000006    |22           |-0.01876277272727273 |\n",
      "|2024-01-03  |AAPL  |-0.9623029999999999     |23           |-0.041839260869565215|\n",
      "|2024-01-03  |BA    |0.21677                 |1            |0.21677              |\n",
      "|2024-01-03  |GOOG  |0.9924640000000001      |11           |0.09022400000000001  |\n",
      "|2024-01-03  |NVDA  |1.295472                |16           |0.080967             |\n",
      "|2024-01-03  |O     |0.254714                |2            |0.127357             |\n",
      "|2024-01-03  |TSLA  |0.19152499999999997     |19           |0.010080263157894736 |\n",
      "|2024-01-04  |AAPL  |0.35792399999999924     |14           |0.025565999999999946 |\n",
      "|2024-01-04  |BA    |0.10101                 |1            |0.10101              |\n",
      "|2024-01-04  |GOOG  |-0.1607269999999999     |11           |-0.014611545454545445|\n",
      "+------------+------+------------------------+-------------+---------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:09:34.333556Z",
     "start_time": "2025-06-23T19:09:33.353339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "pandas_df = agg_df.toPandas()\n",
    "\n",
    "# Save to CSV\n",
    "output_path = os.path.join(output_dir, \"ticker_sentiment_by_day.csv\")\n",
    "pandas_df.to_csv(output_path, index=False, sep=\";\")\n",
    "\n",
    "print(f\"CSV written to {output_path}\")"
   ],
   "id": "1852882eaad320d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV written to output\\ticker_sentiment_by_day.csv\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T19:15:29.234850Z",
     "start_time": "2025-06-23T19:13:27.508512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize InfluxDB client and writer\n",
    "influx_client = InfluxDBClient(\n",
    "    url=INFLUXDB_URL,\n",
    "    token=INFLUXDB_TOKEN,\n",
    "    org=INFLUXDB_ORG\n",
    ")\n",
    "write_api = influx_client.write_api(write_options=SYNCHRONOUS)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "summary_df = agg_df.toPandas()\n",
    "\n",
    "# Optional: calculate median if not part of aggregation\n",
    "if \"median_sentiment_score\" not in summary_df.columns:\n",
    "    summary_df[\"median_sentiment_score\"] = None  # or compute separately\n",
    "\n",
    "# Write each row to InfluxDB\n",
    "for _, row in summary_df.iterrows():\n",
    "    point = (\n",
    "        Point(\"sentiment_data\")\n",
    "        .tag(\"ticker\", row[\"ticker\"])\n",
    "        .tag(\"aggregation\", \"daily\")\n",
    "        .field(\"avg_sentiment_score\", row[\"daily_sentiment\"])\n",
    "        .field(\"median_sentiment_score\", row.get(\"median_sentiment_score\", 0) or 0)\n",
    "        .field(\"article_count\", int(row[\"article_count\"]))\n",
    "        .time(pd.to_datetime(row[\"article_date\"]), WritePrecision.S)\n",
    "    )\n",
    "    write_api.write(bucket=INFLUXDB_BUCKET, org=INFLUXDB_ORG, record=point)\n",
    "\n",
    "print(\"‚úÖ All points written to InfluxDB.\")"
   ],
   "id": "617d061c629ded7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All points written to InfluxDB.\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "spark.stop()"
   ],
   "id": "62bfed7421bc792d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
