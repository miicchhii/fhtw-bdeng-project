{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Spark Sentiment Daily Aggregation\n",
    "\n",
    "We read all json files from the S3 bucket with path /sentiment/\n",
    "\n",
    "These files contain the sentiment_scores and relevance_scores calculated using FinBert per article.\n",
    "\n",
    "The goal is to aggregate the sentiments of all the articles by ticker and day to get a weighted daily sentiment for each ticker.\n",
    "\n",
    "These weighted daily sentiment scores are then stored in InfluxDB to be available for further analysis."
   ],
   "id": "c9ea085a615c37b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import dependencies",
   "id": "ca62ce5d175013a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import explode, col, to_date, avg, sum as _sum, count\n",
    "import boto3\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from influxdb_client import InfluxDBClient, Point, WritePrecision\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime"
   ],
   "id": "e9424e578f9b22ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Configure services",
   "id": "54fbb43b6b1e682e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# üìç InfluxDB-Konfiguration\n",
    "INFLUXDB_URL = \"http://localhost:10896\"\n",
    "INFLUXDB_TOKEN = \"14iJvsBJKp37nLXjIZvE4RbAoEO2dNs1k0GvCbKuJUnF_ub4pSWWw80O739jabLPMD-XBzA72WSX9f-4FuDBQ==\"\n",
    "INFLUXDB_ORG = \"bdinf-org\"\n",
    "INFLUXDB_BUCKET = \"bdinf-bucket\"\n",
    "\n",
    "s3_endpoint_url=\"http://172.29.16.105:9000\"\n",
    "s3_access_key_id=\"bdenggroup3\"\n",
    "s3_secret_access_key=\"bdenggroup3\"\n",
    "bucket_name = \"bdenggroup3\"\n",
    "s3_prefix = \"sentiment\"\n",
    "\n",
    "spark_master_url = \"spark://localhost:7077\""
   ],
   "id": "ac4c2d0ef8626bcb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SparkSession mit MinIO S3-Kompatibilit√§t",
   "id": "8ca28b07dee156d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sentiment Aggregator\") \\\n",
    "    .master(spark_master_url) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ],
   "id": "4779e00686f52860"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Set up boto3 client",
   "id": "ba3c577b2464f2d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "s3 = boto3.client(\"s3\",\n",
    "                  endpoint_url=s3_endpoint_url,\n",
    "                  aws_access_key_id=s3_access_key_id,\n",
    "                  aws_secret_access_key=s3_secret_access_key\n",
    "                  )\n"
   ],
   "id": "a27533a95bad912a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Get a list of all keys (s3 file references)",
   "id": "20c314bf6816d648"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === List all JSON files under the prefix ===\n",
    "keys = []\n",
    "continuation_token = None\n",
    "\n",
    "while True:\n",
    "    if continuation_token:\n",
    "        response = s3.list_objects_v2(\n",
    "            Bucket=bucket_name, Prefix=s3_prefix, ContinuationToken=continuation_token)\n",
    "    else:\n",
    "        response = s3.list_objects_v2(\n",
    "            Bucket=bucket_name, Prefix=s3_prefix)\n",
    "\n",
    "    for obj in response.get(\"Contents\", []):\n",
    "        key = obj[\"Key\"]\n",
    "        if key.endswith(\".json\"):\n",
    "            keys.append(key)\n",
    "\n",
    "    if response.get(\"IsTruncated\"):  # more data available\n",
    "        continuation_token = response.get(\"NextContinuationToken\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(f\"Found {len(keys)} JSON files\")"
   ],
   "id": "dab5b41da6845c2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Read and parse JSON files",
   "id": "5afbbdc2b20effa1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "key_counter = 0\n",
    "rows = []\n",
    "\n",
    "for key in keys:\n",
    "    key_counter += 1\n",
    "    try:\n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "        data = json.load(obj[\"Body\"])\n",
    "        article_timestamp = data.get(\"articleTimestamp\")\n",
    "        url = data.get(\"url\", \"\")\n",
    "        if not article_timestamp:\n",
    "            continue\n",
    "        article_date = article_timestamp[:10]  # 'YYYY-MM-DD'\n",
    "        domain = urlparse(url).netloc.lower()\n",
    "\n",
    "        tickers = data.get(\"tickers\", [])\n",
    "        for ticker in tickers:\n",
    "            sentiment = ticker.get(\"sentiment_score\")\n",
    "            relevance = ticker.get(\"relevance_score\")\n",
    "            symbol = ticker.get(\"ticker\")\n",
    "            if sentiment is not None and relevance is not None and symbol:\n",
    "                weighted_sentiment = sentiment * relevance\n",
    "                rows.append(Row(\n",
    "                    article_date=article_date,\n",
    "                    ticker=symbol,\n",
    "                    sentiment=sentiment,\n",
    "                    relevance=relevance,\n",
    "                    weighted_sentiment=weighted_sentiment,\n",
    "                    domain=domain\n",
    "                ))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {key}: {e}\")\n",
    "\n",
    "    if key_counter % 100 == 0:\n",
    "        print(f\"Processed {key_counter} JSON files\")\n",
    "\n",
    "    if key_counter % 100000 == 0:\n",
    "        print(f\"Cancelled after {key_counter} JSON files\")\n",
    "        break\n",
    "\n",
    "# Optional final update\n",
    "print(f\"Finished processing {key_counter} JSON files total.\")"
   ],
   "id": "ad777417fb78da22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Aggregate Sentiments",
   "id": "f4b6697686dd8394"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create Spark DataFrame from rows\n",
    "df = spark.createDataFrame(rows)\n",
    "\n",
    "# Filter to specific date and select unique domains\n",
    "target_date = \"2025-06-21\"\n",
    "\n",
    "domains_df = df.filter(col(\"article_date\") != target_date).select(\"domain\").distinct()\n",
    "\n",
    "# Show the result\n",
    "domains_df.show(truncate=False)\n",
    "\n",
    "# Optional: Save to file\n",
    "domains_df.toPandas().to_csv(f\"domains_{target_date}.csv\", index=False)\n",
    "\n",
    "# === Create DataFrame and compute relevance-weighted average ===\n",
    "df = spark.createDataFrame(rows)\n",
    "\n",
    "agg_df = df.groupBy(\"article_date\", \"ticker\").agg(\n",
    "    _sum(\"weighted_sentiment\").alias(\"total_weighted_sentiment\"),\n",
    "    count(\"*\").alias(\"article_count\")\n",
    ")\n",
    "\n",
    "# Compute daily average sentiment per ticker\n",
    "agg_df = agg_df.withColumn(\n",
    "    \"daily_sentiment\",\n",
    "    col(\"total_weighted_sentiment\") / col(\"article_count\")\n",
    ")\n",
    "\n",
    "agg_df.orderBy(\"article_date\", \"ticker\").show(truncate=False)"
   ],
   "id": "bba709e5a0327ee0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Save to csv for easier sharing",
   "id": "932a7164c24ad8ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "pandas_df = agg_df.toPandas()\n",
    "\n",
    "# Save to CSV\n",
    "output_path = os.path.join(output_dir, \"ticker_sentiment_by_day.csv\")\n",
    "pandas_df.to_csv(output_path, index=False, sep=\";\")\n",
    "\n",
    "print(f\"CSV written to {output_path}\")"
   ],
   "id": "1852882eaad320d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Store results in InfluxDB",
   "id": "7c2c93659a4ec289"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize InfluxDB client and writer\n",
    "influx_client = InfluxDBClient(\n",
    "    url=INFLUXDB_URL,\n",
    "    token=INFLUXDB_TOKEN,\n",
    "    org=INFLUXDB_ORG\n",
    ")\n",
    "write_api = influx_client.write_api(write_options=SYNCHRONOUS)\n",
    "\n",
    "# Convert Spark DataFrame to Pandas\n",
    "summary_df = agg_df.toPandas()\n",
    "\n",
    "# Optional: calculate median if not part of aggregation\n",
    "if \"median_sentiment_score\" not in summary_df.columns:\n",
    "    summary_df[\"median_sentiment_score\"] = None  # or compute separately\n",
    "\n",
    "# Write each row to InfluxDB\n",
    "for _, row in summary_df.iterrows():\n",
    "    point = (\n",
    "        Point(\"sentiment_data\")\n",
    "        .tag(\"ticker\", row[\"ticker\"])\n",
    "        .tag(\"aggregation\", \"daily\")\n",
    "        .field(\"avg_sentiment_score\", row[\"daily_sentiment\"])\n",
    "        .field(\"median_sentiment_score\", row.get(\"median_sentiment_score\", 0) or 0)\n",
    "        .field(\"article_count\", int(row[\"article_count\"]))\n",
    "        .time(pd.to_datetime(row[\"article_date\"]), WritePrecision.S)\n",
    "    )\n",
    "    write_api.write(bucket=INFLUXDB_BUCKET, org=INFLUXDB_ORG, record=point)\n",
    "\n",
    "print(\"‚úÖ All points written to InfluxDB.\")"
   ],
   "id": "617d061c629ded7a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Stop Spark Session",
   "id": "91a15fda2c52d0aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "spark.stop()",
   "id": "62bfed7421bc792d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
