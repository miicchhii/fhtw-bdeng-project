{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Imports",
   "id": "6f7dce9a4ca720a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timezone"
   ],
   "id": "c8235a0378f96924",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Connection to MinIO",
   "id": "ea9b9434ac479c85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "s3_endpoint_url=\"http://172.29.16.105:9000\"\n",
    "s3_access_key_id=\"bdenggroup3\"\n",
    "s3_secret_access_key=\"bdenggroup3\"\n",
    "bucket_name = \"bdenggroup3\"\n",
    "\n",
    "spark_master_url = \"spark://localhost:7077\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Access Logs Minio\") \\\n",
    "    .master(spark_master_url) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", s3_endpoint_url) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", s3_access_key_id) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", s3_secret_access_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()"
   ],
   "id": "3b73cd02b8110e11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Parser",
   "id": "73e3180b9b12c01f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## HTML Extraction Helper\n",
    "### fool.com"
   ],
   "id": "d013c83831e03abe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_fool_article_data(html: str) -> tuple[str, str]:\n",
    "    article_text = extract_fool_article_text(html)\n",
    "    publish_date = extract_fool_publish_date(html)\n",
    "    return article_text, publish_date\n"
   ],
   "id": "864ddc8b660439af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_fool_article_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    article_body = soup.find(\"div\", class_=\"article-body\")\n",
    "    if article_body:\n",
    "        tags = article_body.find_all(['p', 'h2'])\n",
    "        flat_text = ' '.join(tag.get_text(strip=True) for tag in tags)\n",
    "        return re.sub(r'\\s+', ' ', flat_text).strip()\n",
    "    return \"\"\n"
   ],
   "id": "dad9413a56190723",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_fool_publish_date(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    meta_tag = soup.find(\"meta\", attrs={\"property\": \"article:published_time\"})\n",
    "    if meta_tag and meta_tag.get(\"content\"):\n",
    "        dt = datetime.fromisoformat(meta_tag[\"content\"])\n",
    "        dt_utc = dt.astimezone(timezone.utc)\n",
    "        return dt_utc.replace(tzinfo=None).isoformat(timespec=\"microseconds\")\n",
    "\n",
    "    return \"\""
   ],
   "id": "db9951897483acaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### benzinga.com",
   "id": "66d600e399352a73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_benzinga_article_data(html: str) -> tuple[str, str]:\n",
    "    article_text = extract_benzinga_article_text(html)\n",
    "    publish_date = extract_benzinga_publish_date(html)\n",
    "    return article_text, publish_date"
   ],
   "id": "350ee6f05a394db6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_benzinga_article_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    article_body = soup.find(\"div\", id=\"article-body\")\n",
    "    if article_body:\n",
    "        tags = article_body.find_all(['p', 'h2', 'li'])\n",
    "        flat_text = ' '.join(tag.get_text(strip=True) for tag in tags)\n",
    "        return re.sub(r'\\s+', ' ', flat_text).strip()\n",
    "    return \"\""
   ],
   "id": "8529126208f6642a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_benzinga_publish_date(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    date_span = soup.find(\"span\", class_=\"article-date\")\n",
    "    if not date_span:\n",
    "        return \"\"\n",
    "\n",
    "    raw_date = date_span.get_text(strip=True)\n",
    "\n",
    "    dt = datetime.strptime(raw_date, \"%B %d, %Y %I:%M %p\")\n",
    "    return dt.isoformat(timespec=\"microseconds\")"
   ],
   "id": "b5b4812276be83a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### zacks.com",
   "id": "807e68fc892d64f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_zacks_article_text(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    article_body = soup.find(\"div\", id=\"comtext\")\n",
    "\n",
    "    if article_body:\n",
    "        tags = article_body.find_all(['p', 'h2', 'li'])\n",
    "        flat_text = ' '.join(tag.get_text(strip=True) for tag in tags)\n",
    "        flat_text = re.sub(r'-{5,}.*?-{5,}', '', flat_text)  # Remove ad block content\n",
    "        return re.sub(r'\\s+', ' ', flat_text).strip()\n",
    "\n",
    "    return \"\""
   ],
   "id": "3977b607f4846112",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_zacks_publish_date(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Look for <p class=\"byline\"> and then find the <time> tag inside\n",
    "    byline = soup.find(\"p\", class_=\"byline\")\n",
    "    if byline:\n",
    "        time_tag = byline.find(\"time\")\n",
    "        if time_tag and time_tag.text.strip():\n",
    "            try:\n",
    "                # Example: \"November 04, 2024\"\n",
    "                dt = datetime.strptime(time_tag.text.strip(), \"%B %d, %Y\")\n",
    "                return dt.isoformat(timespec=\"microseconds\")\n",
    "            except ValueError as e:\n",
    "                print(f\"Date parse error: {e}\")\n",
    "\n",
    "    # fallback if not found or error\n",
    "    return datetime.now().isoformat(timespec=\"microseconds\")"
   ],
   "id": "ca3ea7bd2ce14ae4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_zacks_article_data(html: str) -> tuple[str, str]:\n",
    "    article_text = extract_zacks_article_text(html)\n",
    "    publish_date = extract_zacks_publish_date(html)\n",
    "    return article_text, publish_date"
   ],
   "id": "840b3b9cf5b42909",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dynamic Dispatcher",
   "id": "22c2ae627f8ba799"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_article_data_by_source(url: str, html: str) -> tuple[str, str]:\n",
    "    if \"fool.com\" in url:\n",
    "        return extract_fool_article_data(html)\n",
    "    elif \"benzinga.com\" in url:\n",
    "        return extract_benzinga_article_data(html)\n",
    "    elif \"zacks.com\" in url:\n",
    "        return extract_zacks_article_data(html)\n",
    "    else:\n",
    "        return \"\"\n"
   ],
   "id": "2502f3e5eaead16a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Processing Function for Spark workers",
   "id": "6beb8b15bbe41642"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_key(key):\n",
    "    import boto3\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "\n",
    "    s3 = boto3.client(\"s3\",\n",
    "                      endpoint_url=s3_endpoint_url,\n",
    "                      aws_access_key_id=s3_access_key_id,\n",
    "                      aws_secret_access_key=s3_secret_access_key)\n",
    "\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "    data = json.load(obj['Body'])\n",
    "\n",
    "    html = data.get(\"html\", \"\")\n",
    "    url = data.get(\"url\", \"\")\n",
    "    scraping_timestamp = data.get(\"timestamp\", \"\")\n",
    "\n",
    "    article_text, published_date = extract_article_data_by_source(url, html)\n",
    "    parsed_data = {\n",
    "        \"url\": url,\n",
    "        \"scrapingTimestamp\": scraping_timestamp,\n",
    "        \"parsingTimestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"articleTimestamp\": published_date,\n",
    "        \"articleText\": article_text\n",
    "    }\n",
    "\n",
    "    new_key = key.replace(\"raw/scrape_raw_\", \"parsed/parsed_\")\n",
    "\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=new_key,\n",
    "        Body=json.dumps(parsed_data, ensure_ascii=False, indent=2).encode(\"utf-8\"),\n",
    "        ContentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    return f\"parsed {key} â†’ {new_key}\""
   ],
   "id": "8caefd3b53f501fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -- Step 1: List keys from S3 in the 'raw/' folder\n",
    "def list_s3_keys(bucket_name, prefix, s3_client):\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    keys = []\n",
    "    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            if key.startswith(\"raw/scrape_raw_\") and key.endswith(\".json\"):\n",
    "                keys.append(key)\n",
    "    return keys"
   ],
   "id": "1a36af4e3fc082e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -- Step 2: Create boto3 client and list keys\n",
    "s3 = boto3.client(\"s3\",\n",
    "                  endpoint_url=s3_endpoint_url,\n",
    "                  aws_access_key_id=s3_access_key_id,\n",
    "                  aws_secret_access_key=s3_secret_access_key)\n",
    "\n",
    "all_keys = list_s3_keys(bucket_name, \"raw/\", s3)\n",
    "\n",
    "# parse up to 100000 files\n",
    "rdd = spark.sparkContext.parallelize(all_keys[0:100000])\n",
    "saved_keys = rdd.map(process_key).collect()"
   ],
   "id": "efe2ea7282eba43e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "ffe92716091d20e7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
