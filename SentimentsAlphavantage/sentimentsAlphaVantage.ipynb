{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de2adbe1-7964-4c7b-9c99-64e6d15662dd",
   "metadata": {},
   "source": [
    "# Spark Exercise\n",
    "\n",
    "Apache Spark is an excellent tool for data engineering projects due to its robust ability to process large-scale data efficiently through distributed computing. Spark's in-memory processing capabilities significantly enhance the speed of data operations, making it ideal for handling big data workloads. It supports various data sources and formats, offering versatility in data ingestion and transformation. Additionally, Spark's rich API supports multiple programming languages such as Python, Java, and Scala, catering to diverse developer preferences. Its ecosystem, which includes libraries for SQL, machine learning, and graph processing, provides a comprehensive suite for building complex data pipelines and analytics, making it a powerful and flexible choice for data engineering tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531b1187-dab5-46ec-be83-556037eb7b20",
   "metadata": {},
   "source": [
    "Use Python, ```pyspark``` and ```pandas``` to explore Apache Spark RDD and DataFrame:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d57a83-9f00-4b12-861c-640f3187fd42",
   "metadata": {},
   "source": [
    "# Spark RDD\n",
    "\n",
    "Spark RDD (Resilient Distributed Dataset) is a fundamental data structure in Apache Spark that enables fault-tolerant, distributed processing of large datasets across multiple nodes in a cluster. Spark RDDs provide a higher-level abstraction for performing distributed data processing tasks, including both map (transformations) and reduce (aggregations) operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99852f03-87e2-403a-88e9-ca1770ac05da",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "a307a318-4ffa-4b57-98bb-8f011ec7f6ae",
   "metadata": {},
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "75fdeda4-ddd2-47f7-9f4c-7b1f1e2cc957",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T06:30:30.495344Z",
     "start_time": "2025-06-24T06:30:30.489073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# üìç InfluxDB-Konfiguration\n",
    "INFLUXDB_URL = \"http://localhost:10896\"\n",
    "INFLUXDB_TOKEN = \"14iJvsBJKp37nLXjIZvE4RbAoEO2dNs1k0GvCbKuJUnF_ub4pSWWw80O739jabLPMD-XBzA72WSX9f-4FuDBQ==\"\n",
    "INFLUXDB_ORG = \"bdinf-org\"\n",
    "INFLUXDB_BUCKET = \"bdinf-bucket\"\n",
    "\n",
    "spark_master_url = \"spark://localhost:7077\""
   ],
   "id": "cd2e596308cc29d5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "c0408145-2924-4059-8a85-dbd974095d81",
   "metadata": {},
   "source": [
    "## Spark Context and Session\n",
    "Initialize Spark Context and Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "id": "76eb6765-6776-40d0-8d52-1e832fef01f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T06:31:16.227363Z",
     "start_time": "2025-06-24T06:30:38.420355Z"
    }
   },
   "source": [
    "# üß† SparkSession mit MinIO S3-Kompatibilit√§t\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sentiment AlphaVantage\") \\\n",
    "    .master(spark_master_url) \\\n",
    "    .getOrCreate()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "aa2597f6-b983-4042-8918-b7c30edba96b",
   "metadata": {},
   "source": [
    "## Load Data into RDD"
   ]
  },
  {
   "cell_type": "code",
   "id": "c7c42a59-a103-45fe-9ec0-d8d98b88de9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T07:05:40.590498Z",
     "start_time": "2025-06-24T07:05:36.954112Z"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the directory path\n",
    "data_dir = \"data\"\n",
    "\n",
    "# Initialize an empty list to hold all the combined data\n",
    "combined_data = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        with open(file_path, \"r\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                combined_data += data\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding {filename}, skipping.\")\n",
    "\n",
    "# `combined_data` now contains all data from the JSON files\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "deaf7757-8175-41fc-b085-5ee3c4e4cc51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T07:05:44.093880Z",
     "start_time": "2025-06-24T07:05:43.910855Z"
    }
   },
   "source": [
    "rdd = spark.sparkContext.parallelize(data)\n",
    "parsed_rdd = rdd.map(lambda x: json.loads(x))"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "f8ac558b-97aa-4705-8052-b81d73b74774",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T07:05:45.701519Z",
     "start_time": "2025-06-24T07:05:45.688347Z"
    }
   },
   "source": [
    "print (type(parsed_rdd))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.core.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "6261385f-aba5-4063-9185-a1208f21bc14",
   "metadata": {},
   "source": [
    "## Map Operation\n",
    "\n",
    "Split data into individual parts and create key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "id": "3774b927-cb7d-4885-8970-864490e0e9a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T07:05:52.385279Z",
     "start_time": "2025-06-24T07:05:52.360240Z"
    }
   },
   "source": [
    "mapped_rdd = rdd.flatMap(lambda row: [\n",
    "    (\n",
    "        (row['time_published'][:8], ts['ticker']),  # key: (date, ticker)\n",
    "        (\n",
    "            float(ts['ticker_sentiment_score']),            # sentiment\n",
    "            float(ts['relevance_score']),                   # relevance\n",
    "            1,                                              # count\n",
    "            float(ts['ticker_sentiment_score']) * float(ts['relevance_score'])  # sentiment * relevance\n",
    "        )\n",
    "    )\n",
    "    for ts in row.get('ticker_sentiment', [])\n",
    "    if ts.get('ticker') in ['AAPL', 'GOOG', 'BA', 'NVDA', 'O', 'TSLA']\n",
    "])"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "32251e9d-3f2a-4fc2-b9a4-ef5a1f94ea0c",
   "metadata": {},
   "source": [
    "## Reduce Operation\n",
    "\n",
    "Reduce your key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "id": "ef497dcd-5b23-491b-8e07-73a6fef923df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T07:07:01.586511Z",
     "start_time": "2025-06-24T07:07:01.543841Z"
    }
   },
   "source": [
    "#Reduce (sum up sentiment, relevance, count)\n",
    "reduced_rdd = mapped_rdd.reduceByKey(\n",
    "    lambda a, b: (\n",
    "        a[0] + b[0],  # sentiment sum\n",
    "        a[1] + b[1],  # relevance sum\n",
    "        a[2] + b[2],   # count\n",
    "        a[3] + b[3]   # sum of (sentiment * relevance)\n",
    "    )\n",
    ")\n",
    "\n",
    "#Compute Averages\n",
    "final_rdd = reduced_rdd.mapValues(lambda x: (\n",
    "    x[0] / x[2],  # avg sentiment\n",
    "    x[1] / x[2],   # avg relevance\n",
    "    x[3] / x[1] if x[1] != 0 else 0\n",
    "))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "901c597c-a9a2-489f-9399-4be9a5af7e78",
   "metadata": {},
   "source": [
    "## Collect Results\n",
    "\n",
    "Because of lazy evaluation, the map-reduce operation is performed only now. Show what you calculated."
   ]
  },
  {
   "cell_type": "code",
   "id": "46ec9af1-eb46-40cd-9d58-352e70599d44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T07:08:02.445678Z",
     "start_time": "2025-06-24T07:08:00.561186Z"
    }
   },
   "source": [
    "# Convert to flat records: (ticker, date, avg_sentiment, avg_relevance)\n",
    "formatted_rdd = final_rdd.map(lambda x: (\n",
    "    x[0][1],  # ticker\n",
    "    x[0][0],  # date\n",
    "    x[1][0],  # avg sentiment\n",
    "    x[1][1],  # avg relevance\n",
    "    x[1][2]   # weighted sentiment\n",
    "))\n",
    "\n",
    "results = formatted_rdd.collect()\n",
    "\n",
    "# Print a few rows\n",
    "for row in results[:5]:\n",
    "    print(row)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GOOG', '20240122', 0.18401800000000001, 0.1609795, 0.16978489321388748)\n",
      "('TSLA', '20240123', 0.08054454237288135, 0.29334440677966117, 0.10585794423602263)\n",
      "('BA', '20240124', -0.06395, 0.097566, -0.05961403160937211)\n",
      "('BA', '20240128', -0.1025755, 0.12291149999999999, -0.07014408278314072)\n",
      "('NVDA', '20240129', 0.13267408695652175, 0.1503447391304348, 0.12140735311424843)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "c724a926-8e29-459f-96d0-51d202ff14cb",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "id": "19db77f4-03a1-446e-9b24-4193fc55fb00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T07:12:24.983014Z",
     "start_time": "2025-06-24T07:12:24.853781Z"
    }
   },
   "source": [
    "import csv\n",
    "\n",
    "# Prepare your data: list of dicts with all 5 fields\n",
    "csv_data = [\n",
    "    {\n",
    "        \"ticker\": t,\n",
    "        \"date\": d,\n",
    "        \"avg_sentiment\": s,\n",
    "        \"avg_relevance\": r,\n",
    "        \"weighted_sentiment\": w\n",
    "    }\n",
    "    for t, d, s, r, w in results\n",
    "]\n",
    "\n",
    "# Define output path\n",
    "output_csv_path = \"output/ticker_by_day_sentiment_av.csv\"\n",
    "\n",
    "# Write to CSV\n",
    "with open(output_csv_path, \"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"ticker\", \"date\", \"avg_sentiment\", \"avg_relevance\", \"weighted_sentiment\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(csv_data)\n"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Convert RDD to Spark DataFrame",
   "id": "7149bf49962040c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T07:19:32.052678Z",
     "start_time": "2025-06-24T07:19:29.964596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Define schema for DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ticker\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"avg_sentiment\", FloatType(), True),\n",
    "    StructField(\"avg_relevance\", FloatType(), True),\n",
    "    StructField(\"weighted_sentiment\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(formatted_rdd, schema)\n"
   ],
   "id": "7479251787f199e1",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T07:20:15.267672Z",
     "start_time": "2025-06-24T07:20:08.364230Z"
    }
   },
   "cell_type": "code",
   "source": "summary_df = df.toPandas()",
   "id": "5f059e69e9cfd43b",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T07:24:01.057351Z",
     "start_time": "2025-06-24T07:22:30.746681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from influxdb_client import InfluxDBClient, Point, WritePrecision\n",
    "from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "import pandas as pd\n",
    "\n",
    "# InfluxDB setup\n",
    "influx_client = InfluxDBClient(\n",
    "    url=INFLUXDB_URL,\n",
    "    token=INFLUXDB_TOKEN,\n",
    "    org=INFLUXDB_ORG\n",
    ")\n",
    "write_api = influx_client.write_api(write_options=SYNCHRONOUS)\n",
    "\n",
    "# Write each row to Influx\n",
    "for _, row in summary_df.iterrows():\n",
    "    point = (\n",
    "        Point(\"sentiment_alphavantage\")\n",
    "        .tag(\"ticker\", row[\"ticker\"])\n",
    "        .tag(\"aggregation\", \"daily\")\n",
    "        .field(\"avg_sentiment\", row[\"avg_sentiment\"])\n",
    "        .field(\"avg_relevance\", row[\"avg_relevance\"])\n",
    "        .field(\"weighted_sentiment\", row[\"weighted_sentiment\"])\n",
    "        .time(pd.to_datetime(row[\"date\"]), WritePrecision.S)\n",
    "    )\n",
    "    write_api.write(bucket=INFLUXDB_BUCKET, org=INFLUXDB_ORG, record=point)\n",
    "\n",
    "print(\"‚úÖ All sentiment points written to InfluxDB.\")"
   ],
   "id": "7aed8ce9fc063061",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All sentiment points written to InfluxDB.\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "467981e5-9c6b-4166-b418-61e3ce3f50b1",
   "metadata": {},
   "source": [
    "spark.stop()\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
